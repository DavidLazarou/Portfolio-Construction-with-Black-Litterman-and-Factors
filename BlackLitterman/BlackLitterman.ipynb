{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-20T07:58:39.565845Z",
     "start_time": "2025-08-20T07:58:35.844268Z"
    }
   },
   "source": [
    "# step1_prep_prices.py\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "# --- Asset Universes ---\n",
    "SEMIS = [\n",
    "    \"NVDA\",\"AMD\",\"AVGO\",\"QCOM\",\"TSM\",\"ASML\",\"LRCX\",\"TXN\",\"INTC\",\"ADI\",\n",
    "    \"MRVL\",\"KLAC\",\"LSCC\",\"GFS\",\"AMAT\",\"ALGM\",\"CRDO\",\"RMBS\",\n",
    "    \"TER\",\"COHU\",\"ENTG\",\"ICHR\",\"SNPS\",\"CDNS\",\"ANSS\"\n",
    "]\n",
    "\n",
    "# --- Diversifier Universe ---\n",
    "# US Treasury Bonds, Gold, Copper, Biotech, Vix, Investment Grade Credit\n",
    "\n",
    "DIVERSIFIERS = [\"TLT\",\"GLD\",\"CPER\",\"USO\",\"XBI\",\"VIXY\",\"LQD\"]\n",
    "\n",
    "# Synthetic \"short\" tickers (just aliases for testing diversifiying qualities of macro shorts)\n",
    "\n",
    "SHORTS = {\"SHORT_CPER\": \"CPER\", \"SHORT_USO\": \"USO\", \"SHORT_GLD\": \"GLD\"}\n",
    "\n",
    "# Extra tickers for custom factor construction as follows\n",
    "# ETF to use as Benchmark for Semis (SMH)\n",
    "# ETF to use as Benchmark for Software (SMH)\n",
    "# ETF to use as Benchmark for Cloud (SKYY)\n",
    "# Named stocks to form the Hyperscaler basket: Microsoft, Amazon, GOOGL\n",
    "\n",
    "FACTOR_TICKERS = [\"SMH\", \"IGV\", \"SKYY\", \"MSFT\", \"AMZN\", \"GOOGL\", \"SPY\"]\n",
    "\n",
    "# Combined ticker set for raw data pulls\n",
    "TICKERS = SEMIS + DIVERSIFIERS + list(SHORTS.values()) + FACTOR_TICKERS\n",
    "\n",
    "YEARS = 3\n",
    "MIN_DAYS = 500\n",
    "TRADING_DAYS = 252\n",
    "OUTDIR = Path(\"outputs\"); OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Industry Classification mapping ---\n",
    "CLASSIFICATION = {\n",
    "    # Semiconductor Equipment\n",
    "    \"AMAT\": (\"Applied Materials\", \"Semiconductor Equipment\", \"Wafer Fab Equipment\"),\n",
    "    \"LRCX\": (\"Lam Research\", \"Semiconductor Equipment\", \"Wafer Fab Equipment\"),\n",
    "    \"KLAC\": (\"KLA\", \"Semiconductor Equipment\", \"Process Control\"),\n",
    "    \"ASML\": (\"ASML Holding\", \"Semiconductor Equipment\", \"Lithography\"),\n",
    "    \"TER\": (\"Teradyne\", \"Semiconductor Equipment\", \"Test Equipment\"),\n",
    "    \"COHU\": (\"Cohu\", \"Semiconductor Equipment\", \"Test Equipment\"),\n",
    "    \"ENTG\": (\"Entegris\", \"Semiconductor Equipment\", \"Materials/Consumables\"),\n",
    "    \"ICHR\": (\"Ichor\", \"Semiconductor Equipment\", \"Subsystems\"),\n",
    "    # Semiconductors Core\n",
    "    \"INTC\": (\"Intel\", \"Semiconductors\", \"IDM\"),\n",
    "    \"TSM\": (\"Taiwan Semi\", \"Semiconductors\", \"Foundry\"),\n",
    "    \"TXN\": (\"Texas Instruments\", \"Semiconductors\", \"Analog IDM\"),\n",
    "    \"ADI\": (\"Analog Devices\", \"Semiconductors\", \"Analog IDM\"),\n",
    "    \"AVGO\": (\"Broadcom\", \"Semiconductors\", \"Diversified/Fabless\"),\n",
    "    \"QCOM\": (\"Qualcomm\", \"Semiconductors\", \"Wireless Fabless\"),\n",
    "    \"NVDA\": (\"Nvidia\", \"Semiconductors\", \"Fabless (GPUs)\"),\n",
    "    \"AMD\": (\"AMD\", \"Semiconductors\", \"Fabless (CPUs/GPUs)\"),\n",
    "    \"MRVL\": (\"Marvell\", \"Semiconductors\", \"Fabless (Networking/Storage)\"),\n",
    "    \"LSCC\": (\"Lattice Semi\", \"Semiconductors\", \"Fabless (FPGAs)\"),\n",
    "    \"GFS\": (\"GlobalFoundries\", \"Semiconductors\", \"Foundry/IDM\"),\n",
    "    \"ALGM\": (\"Allegro Micro\", \"Semiconductors\", \"Analog Auto/Industrial\"),\n",
    "    \"CRDO\": (\"Credo Tech\", \"Semiconductors\", \"Fabless Networking\"),\n",
    "    \"RMBS\": (\"Rambus\", \"Semiconductors\", \"Fabless IP/Memory\"),\n",
    "    # Electronic Design Automation (EDA)\n",
    "    \"SNPS\": (\"Synopsys\", \"Software (EDA)\", \"EDA\"),\n",
    "    \"CDNS\": (\"Cadence\", \"Software (EDA)\", \"EDA\"),\n",
    "    \"ANSS\": (\"Ansys\", \"Software (EDA)\", \"EDA\")\n",
    "}\n",
    "\n",
    "def fetch_prices(tickers, years=3):\n",
    "    \"\"\"Download adjusted daily close prices for given tickers.\"\"\"\n",
    "    end = datetime.utcnow()\n",
    "    start = end - timedelta(days=int(365.25 * years))\n",
    "    df = yf.download(\n",
    "        tickers, start=start, end=end, interval=\"1d\",\n",
    "        auto_adjust=True, progress=False\n",
    "    )[\"Close\"]\n",
    "    if isinstance(df, pd.Series):\n",
    "        df = df.to_frame()\n",
    "    return df.sort_index().ffill()\n",
    "\n",
    "def compute_stats_and_plots(rets, tickers, suffix):\n",
    "    \"\"\"Compute annualised vols + correlation matrices, save CSVs + plots.\"\"\"\n",
    "    rets_sel = rets[tickers].dropna(axis=1, how=\"any\")\n",
    "    if rets_sel.empty:\n",
    "        return pd.Series(dtype=float), np.nan\n",
    "\n",
    "    # Annualised vol\n",
    "    ann_vol = rets_sel.std() * np.sqrt(TRADING_DAYS)\n",
    "    ann_vol.name = \"AnnVol\"\n",
    "    ann_vol.to_csv(OUTDIR / f\"step1_ann_vol_{suffix}.csv\", header=True)\n",
    "\n",
    "    # --- Plot Top 15 ---\n",
    "    top15 = ann_vol.sort_values(ascending=False).head(15)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    top15.sort_values().plot(kind=\"barh\", color=\"steelblue\")\n",
    "    plt.xlabel(\"Annualised Volatility\")\n",
    "    plt.ylabel(\"Ticker\")\n",
    "    plt.title(f\"Top 15 Annualised Volatilities ({suffix})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTDIR / f\"step1_ann_vol_top15_{suffix}.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # --- Plot All ---\n",
    "    sorted_vols = ann_vol.sort_values(ascending=False)\n",
    "    plt.figure(figsize=(10, max(6, len(sorted_vols) * 0.35)))\n",
    "    sorted_vols.sort_values().plot(kind=\"barh\", color=\"steelblue\")\n",
    "    plt.xlabel(\"Annualised Volatility\")\n",
    "    plt.ylabel(\"Ticker\")\n",
    "    plt.title(f\"All Annualised Volatilities ({suffix})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTDIR / f\"step1_ann_vol_{suffix}.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Correlation matrix + heatmap\n",
    "    corr = rets_sel.corr()\n",
    "    corr.to_csv(OUTDIR / f\"step1_corr_{suffix}.csv\")\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr, cmap=\"coolwarm\", center=0, annot=False, linewidths=0.3)\n",
    "    plt.title(f\"Correlation Matrix ({suffix})\", fontsize=14)\n",
    "    plt.xticks(rotation=90, fontsize=7)\n",
    "    plt.yticks(fontsize=7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTDIR / f\"step1_corr_{suffix}.png\")\n",
    "    plt.close()\n",
    "\n",
    "    offdiag = corr.values[np.triu_indices_from(corr, k=1)]\n",
    "    return ann_vol, np.median(offdiag)\n",
    "\n",
    "def main():\n",
    "    prices = fetch_prices(TICKERS, years=YEARS)\n",
    "\n",
    "    # Keep only tickers with enough history\n",
    "    day_counts = prices.count()\n",
    "    keep = day_counts[day_counts >= MIN_DAYS].index.tolist()\n",
    "    dropped = sorted(set(TICKERS) - set(keep))\n",
    "    prices = prices[keep]\n",
    "\n",
    "    # Returns\n",
    "    rets = prices.pct_change().dropna()\n",
    "\n",
    "    # Save all prices/returns\n",
    "    prices.to_csv(OUTDIR / \"step1_prices.csv\")\n",
    "    rets.to_csv(OUTDIR / \"step1_returns_daily.csv\")\n",
    "\n",
    "    # Save dropped tickers\n",
    "    if dropped:\n",
    "        with open(OUTDIR / \"step1_dropped_tickers.txt\", \"w\") as f:\n",
    "            f.write(\"\\n\".join(dropped))\n",
    "\n",
    "    # Stats + plots: SEMIS (base universe)\n",
    "    ann_vol_semis, medcorr_semis = compute_stats_and_plots(\n",
    "        rets, [t for t in SEMIS if t in rets.columns], \"semis\"\n",
    "    )\n",
    "\n",
    "    # Stats + plots: ALL (semis + diversifiers + shorts only; exclude FACTOR_TICKERS)\n",
    "    core_tickers = [t for t in SEMIS + DIVERSIFIERS + list(SHORTS.values()) if t in rets.columns]\n",
    "    ann_vol_all, medcorr_all = compute_stats_and_plots(\n",
    "        rets, core_tickers, \"all\"\n",
    "    )\n",
    "\n",
    "    # --- Universe classification table ---\n",
    "    records = []\n",
    "    for tkr in SEMIS:\n",
    "        if tkr in CLASSIFICATION:\n",
    "            name, gics, model = CLASSIFICATION[tkr]\n",
    "            records.append({\"Ticker\": tkr, \"Company\": name, \"GICS Sub-Industry\": gics, \"Business Model\": model})\n",
    "    df_class = pd.DataFrame(records)\n",
    "    df_class.to_csv(OUTDIR / \"step1_universe_table.csv\", index=False)\n",
    "\n",
    "    # Also save LaTeX for Overleaf\n",
    "    with open(OUTDIR / \"step1_universe_table.tex\", \"w\") as f:\n",
    "        f.write(df_class.to_latex(index=False, longtable=True, escape=False))\n",
    "\n",
    "    # Console summary\n",
    "    print(f\"\\nâœ“ Kept {len(keep)} tickers, dropped {len(dropped)} (<{MIN_DAYS} days)\")\n",
    "    if dropped:\n",
    "        print(\"Dropped:\", \", \".join(dropped))\n",
    "\n",
    "    print(\"\\nTop 15 by annualised vol (SEMIS, %):\")\n",
    "    print((ann_vol_semis.sort_values(ascending=False).head(15) * 100).round(2).astype(str) + \"%\")\n",
    "\n",
    "    print(f\"\\nMedian off-diagonal correlation (SEMIS): {medcorr_semis:.2f}\")\n",
    "    print(f\"Median off-diagonal correlation (ALL)  : {medcorr_all:.2f}\")\n",
    "\n",
    "    print(\"\\nFiles saved:\")\n",
    "    print(\" - step1_prices.csv, step1_returns_daily.csv\")\n",
    "    print(\" - step1_ann_vol_semis.csv/.png, step1_ann_vol_top15_semis.png, step1_corr_semis.csv/.png\")\n",
    "    print(\" - step1_ann_vol_all.csv/.png, step1_ann_vol_top15_all.png, step1_corr_all.csv/.png\")\n",
    "    print(\" - step1_dropped_tickers.txt\")\n",
    "    print(\" - step1_universe_table.csv/.tex\")\n",
    "    print(\" - (FACTOR_TICKERS downloaded but excluded from stats/plots)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Kept 39 tickers, dropped 0 (<500 days)\n",
      "\n",
      "Top 15 by annualised vol (SEMIS, %):\n",
      "Ticker\n",
      "CRDO    79.92%\n",
      "ICHR    61.56%\n",
      "MRVL    60.62%\n",
      "LSCC    54.47%\n",
      "NVDA    53.19%\n",
      "RMBS    53.02%\n",
      "ENTG    51.83%\n",
      "ALGM    51.24%\n",
      "AMD     51.14%\n",
      "INTC    49.58%\n",
      "AVGO    45.89%\n",
      "TER     44.54%\n",
      "LRCX    43.96%\n",
      "GFS     43.93%\n",
      "COHU    42.19%\n",
      "Name: AnnVol, dtype: object\n",
      "\n",
      "Median off-diagonal correlation (SEMIS): 0.59\n",
      "Median off-diagonal correlation (ALL)  : 0.42\n",
      "\n",
      "Files saved:\n",
      " - step1_prices.csv, step1_returns_daily.csv\n",
      " - step1_ann_vol_semis.csv/.png, step1_ann_vol_top15_semis.png, step1_corr_semis.csv/.png\n",
      " - step1_ann_vol_all.csv/.png, step1_ann_vol_top15_all.png, step1_corr_all.csv/.png\n",
      " - step1_dropped_tickers.txt\n",
      " - step1_universe_table.csv/.tex\n",
      " - (FACTOR_TICKERS downloaded but excluded from stats/plots)\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T20:32:18.835595Z",
     "start_time": "2025-08-19T20:32:18.384392Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# pull in t-bills and 2.compute_excess_returns.py\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas_datareader.data import DataReader\n",
    "\n",
    "IN_RET = Path(\"outputs/step1_returns_daily.csv\")\n",
    "OUTDIR = Path(\"outputs\"); OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def fetch_rf_daily(start, end):\n",
    "    # 3M Treasury Constant Maturity (daily, %)\n",
    "    rf = DataReader(\"DGS3MO\", \"fred\", start, end)\n",
    "    rf = rf.rename(columns={\"DGS3MO\": \"DGS3MO_%\"}).sort_index()\n",
    "    # Convert % annual -> daily decimal (approx 252 trading days)\n",
    "    rf[\"rf_daily\"] = (rf[\"DGS3MO_%\"] / 100.0) / 252.0\n",
    "    # Forward-fill to trading days later\n",
    "    return rf[[\"rf_daily\"]]\n",
    "\n",
    "def main():\n",
    "    # Load daily simple returns from Step 1\n",
    "    rets = pd.read_csv(IN_RET, index_col=0, parse_dates=True)\n",
    "    rets = rets.sort_index()\n",
    "\n",
    "    # Fetch risk-free over same window\n",
    "    rf = fetch_rf_daily(rets.index.min(), rets.index.max())\n",
    "\n",
    "    # Align to trading days and forward-fill gaps (weekends/holidays)\n",
    "    rf_aligned = rf.reindex(rets.index).ffill().fillna(0.0)[\"rf_daily\"]\n",
    "\n",
    "    # Excess returns = asset returns - rf_daily (per column)\n",
    "    excess = rets.sub(rf_aligned, axis=0)\n",
    "\n",
    "    # --- Sanity checks to prevent silent drift ---\n",
    "    assert rets.index.equals(excess.index), \"Index mismatch: returns vs excess returns\"\n",
    "    assert rets.index.equals(rf_aligned.index), \"Index mismatch: returns vs risk-free series\"\n",
    "    assert excess.notna().all().all(), \"NaNs found in excess returns!\"\n",
    "\n",
    "    # Save\n",
    "    rf_aligned.to_frame(name=\"rf_daily\").to_csv(OUTDIR / \"step2_rf_daily.csv\")\n",
    "    excess.to_csv(OUTDIR / \"step2_excess_returns.csv\")\n",
    "\n",
    "    # Console summary\n",
    "    rf_ann = rf_aligned.mean() * 252\n",
    "    print(f\"âœ“ Saved risk-free series â†’ {OUTDIR/'step2_rf_daily.csv'}\")\n",
    "    print(f\"âœ“ Saved excess returns   â†’ {OUTDIR/'step2_excess_returns.csv'}\")\n",
    "    print(f\"Avg annualized RF over sample: {rf_ann:.2%}\")\n",
    "    print(\"Excess returns preview:\\n\", excess.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "bee6cac17a659c21",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Saved risk-free series â†’ outputs/step2_rf_daily.csv\n",
      "âœ“ Saved excess returns   â†’ outputs/step2_excess_returns.csv\n",
      "Avg annualized RF over sample: 4.88%\n",
      "Excess returns preview:\n",
      "                  ADI      ALGM      AMAT       AMD      AMZN      ANSS  \\\n",
      "Date                                                                     \n",
      "2022-08-23  0.003368  0.001917  0.010999 -0.003881  0.002891 -0.013051   \n",
      "2022-08-24 -0.004735  0.010819 -0.001981  0.002483  0.001235  0.004129   \n",
      "2022-08-25  0.020784  0.024315  0.034573  0.047874  0.025895  0.025801   \n",
      "2022-08-26 -0.050393 -0.045854 -0.059162 -0.061856 -0.047682 -0.048391   \n",
      "2022-08-29 -0.017343 -0.025927 -0.022992 -0.029620 -0.007460 -0.012504   \n",
      "\n",
      "                ASML      AVGO      CDNS      COHU  ...       SMH      SNPS  \\\n",
      "Date                                                ...                       \n",
      "2022-08-23  0.010810  0.001837 -0.009035  0.007724  ...  0.006370  0.002402   \n",
      "2022-08-24 -0.000701  0.001889 -0.001531  0.013669  ... -0.000769 -0.000808   \n",
      "2022-08-25  0.027101  0.036224  0.009777  0.063322  ...  0.033464  0.012457   \n",
      "2022-08-26 -0.055326 -0.053320 -0.037831 -0.054523  ... -0.055717 -0.022853   \n",
      "2022-08-29 -0.019846 -0.012943 -0.016988 -0.022302  ... -0.018666 -0.021273   \n",
      "\n",
      "                 SPY       TER       TLT       TSM       TXN       USO  \\\n",
      "Date                                                                     \n",
      "2022-08-23 -0.002530  0.005948 -0.005614  0.009157  0.006188  0.028678   \n",
      "2022-08-24  0.003089 -0.004522 -0.007431 -0.005575 -0.002025  0.015637   \n",
      "2022-08-25  0.014003  0.040504  0.013822  0.023378  0.029620 -0.019848   \n",
      "2022-08-26 -0.033964 -0.064165  0.007423 -0.032432 -0.044218 -0.008611   \n",
      "2022-08-29 -0.006730 -0.028179 -0.008391 -0.022422 -0.009263  0.037325   \n",
      "\n",
      "                VIXY       XBI  \n",
      "Date                            \n",
      "2022-08-23 -0.017234  0.021418  \n",
      "2022-08-24 -0.032864  0.020963  \n",
      "2022-08-25 -0.033976 -0.003015  \n",
      "2022-08-26  0.101302 -0.048565  \n",
      "2022-08-29 -0.009596 -0.012818  \n",
      "\n",
      "[5 rows x 39 columns]\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T20:32:39.414352Z",
     "start_time": "2025-08-19T20:32:38.359062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# step3_covariances.py  (safe Overleaf table patch)\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.covariance import LedoitWolf\n",
    "\n",
    "OUTDIR = Path(\"outputs\")\n",
    "OUTDIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# --- Asset Universes (core groups stay fixed) ---\n",
    "SEMIS = [\n",
    "    \"NVDA\",\"AMD\",\"AVGO\",\"QCOM\",\"TSM\",\"ASML\",\"LRCX\",\"TXN\",\"INTC\",\"ADI\",\n",
    "    \"MRVL\",\"KLAC\",\"LSCC\",\"GFS\",\"AMAT\",\"ALGM\",\"CRDO\",\"RMBS\",\n",
    "    \"TER\",\"COHU\",\"ENTG\",\"ICHR\",\"SNPS\",\"CDNS\",\"ANSS\"\n",
    "]\n",
    "DIVERSIFIERS = [\"TLT\",\"GLD\",\"CPER\",\"USO\",\"XBI\",\"VIXY\",\"LQD\"]\n",
    "\n",
    "# Shorts: synthetic tickers we invert\n",
    "SHORTS = {\"SHORT_CPER\": \"CPER\", \"SHORT_USO\": \"USO\", \"SHORT_GLD\": \"GLD\"}\n",
    "\n",
    "# Report-specific settings (only affect the Overleaf .tex table)\n",
    "REPORT_DIVERSIFIERS = [\"GLD\", \"TLT\", \"VIXY\"]   # only these 3 in the paper\n",
    "REPORT_SORT_BY = \"vol\"  # \"vol\" for volatility (desc) or \"drawdown\" for worst-first\n",
    "\n",
    "def detect_factors(all_tickers):\n",
    "    known = set(SEMIS + DIVERSIFIERS + list(SHORTS.keys()))\n",
    "    return [t for t in all_tickers if t not in known]\n",
    "\n",
    "def diag(mat: pd.DataFrame):\n",
    "    eig = np.linalg.eigvalsh(mat.values)\n",
    "    lam_min, lam_max = float(eig.min()), float(eig.max())\n",
    "    cond = (lam_max / lam_min) if lam_min > 0 else np.inf\n",
    "    return lam_min, lam_max, cond\n",
    "\n",
    "def top_bottom_corr(corr: pd.DataFrame, n=10):\n",
    "    corr_long = (\n",
    "        corr.stack()\n",
    "        .reset_index()\n",
    "        .rename(columns={0: \"Correlation\", \"level_0\": \"Asset1\", \"level_1\": \"Asset2\"})\n",
    "    )\n",
    "    corr_long = corr_long[corr_long[\"Asset1\"] < corr_long[\"Asset2\"]]\n",
    "    return (\n",
    "        corr_long.nsmallest(n, \"Correlation\").reset_index(drop=True),\n",
    "        corr_long.nlargest(n, \"Correlation\").reset_index(drop=True),\n",
    "    )\n",
    "\n",
    "def compute_drawdowns(sub: pd.DataFrame):\n",
    "    dd = {}\n",
    "    for col in sub:\n",
    "        cum = (1 + sub[col]).cumprod()\n",
    "        running_max = cum.cummax()\n",
    "        drawdown = (cum / running_max - 1.0).min()\n",
    "        dd[col] = float(drawdown)\n",
    "    return pd.Series(dd, name=\"max_drawdown\")\n",
    "\n",
    "def process_subset(rets: pd.DataFrame, tickers: list, suffix: str):\n",
    "    sub = rets[[t for t in tickers if t in rets.columns]].dropna(axis=1, how=\"any\")\n",
    "    if sub.empty:\n",
    "        print(f\"âš ï¸ No valid returns for {suffix}\")\n",
    "        return\n",
    "\n",
    "    # Covariance & shrinkage (annualised)\n",
    "    S_sample_ann = sub.cov() * 252.0\n",
    "    lw = LedoitWolf().fit(sub.values)\n",
    "    S_lw_ann = pd.DataFrame(lw.covariance_, index=sub.columns, columns=sub.columns) * 252.0\n",
    "\n",
    "    # Correlation\n",
    "    corr = sub.corr()\n",
    "\n",
    "    # Drawdowns & vol\n",
    "    dd = compute_drawdowns(sub)\n",
    "    vols = sub.std() * np.sqrt(252)\n",
    "    vol_dd = pd.concat([vols.rename(\"Annualised Volatility\"), dd], axis=1)\n",
    "    vol_dd.to_latex(OUTDIR / f\"step3_vol_dd_{suffix}.tex\", float_format=\"%.2f\")\n",
    "\n",
    "    # Heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr, cmap=\"coolwarm\", center=0, annot=False, linewidths=0.3)\n",
    "    plt.title(f\"Correlation Matrix (Excess, {suffix})\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTDIR / f\"step3_corr_{suffix}.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Top/Bottom correlation pairs\n",
    "    bottom, top = top_bottom_corr(corr, n=10)\n",
    "    bottom.to_latex(OUTDIR / f\"step3_lowest_corr_pairs_{suffix}.tex\", index=False, float_format=\"%.2f\")\n",
    "    top.to_latex(OUTDIR / f\"step3_highest_corr_pairs_{suffix}.tex\", index=False, float_format=\"%.2f\")\n",
    "\n",
    "    print(f\"\\nSubset: {suffix} â†’ wrote .tex + .png to outputs/\")\n",
    "\n",
    "def main():\n",
    "    rets = pd.read_csv(OUTDIR/\"step2_excess_returns.csv\", index_col=0, parse_dates=True).sort_index()\n",
    "\n",
    "    # Invert SHORT tickers\n",
    "    for short_name, underlying in SHORTS.items():\n",
    "        if underlying in rets.columns:\n",
    "            rets[short_name] = -rets[underlying]\n",
    "\n",
    "    # Drop NaNs\n",
    "    rets = rets.dropna(how=\"any\")\n",
    "\n",
    "    # Factor detection\n",
    "    FACTORS = detect_factors(rets.columns.tolist())\n",
    "\n",
    "    # --- Original analytics (unchanged) ---\n",
    "    process_subset(rets, [t for t in SEMIS if t in rets.columns], \"semis\")\n",
    "    process_subset(rets, [t for t in DIVERSIFIERS + list(SHORTS.keys()) if t in rets.columns], \"diversifiers\")\n",
    "    if FACTORS:\n",
    "        process_subset(rets, FACTORS, \"factors\")\n",
    "    process_subset(rets, rets.columns.tolist(), \"all\")  # keep original 'all' outputs\n",
    "\n",
    "    # --- Overleaf-specific curated table (does NOT change analytics) ---\n",
    "    # 1) Backup the original 'all' vol/DD table as _full.tex for traceability\n",
    "    try:\n",
    "        # Recompute quickly rather than parsing LaTeX\n",
    "        sub_full = rets[rets.columns].dropna(axis=1, how=\"any\")\n",
    "        vols_full = sub_full.std() * np.sqrt(252)\n",
    "        dd_full = compute_drawdowns(sub_full)\n",
    "        vol_dd_full = pd.concat([vols_full.rename(\"Annualised Volatility\"), dd_full], axis=1)\n",
    "        vol_dd_full.to_latex(OUTDIR / \"step3_vol_dd_all_full.tex\", float_format=\"%.2f\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Could not write step3_vol_dd_all_full.tex backup: {e}\")\n",
    "\n",
    "    # 2) Write curated Semis + GLD/TLT/VIXY to step3_vol_dd_all.tex (the file Overleaf includes)\n",
    "    report_cols = [t for t in (SEMIS + REPORT_DIVERSIFIERS) if t in rets.columns]\n",
    "    sub_rep = rets[report_cols].dropna(axis=1, how=\"any\")\n",
    "    if not sub_rep.empty:\n",
    "        vols_rep = sub_rep.std() * np.sqrt(252)\n",
    "        dd_rep = compute_drawdowns(sub_rep)\n",
    "        vol_dd_rep = pd.concat([vols_rep.rename(\"Annualised Volatility\"), dd_rep], axis=1)\n",
    "\n",
    "        if REPORT_SORT_BY.lower() == \"vol\":\n",
    "            vol_dd_rep = vol_dd_rep.sort_values(by=\"Annualised Volatility\", ascending=False)\n",
    "        elif REPORT_SORT_BY.lower() == \"drawdown\":\n",
    "            vol_dd_rep = vol_dd_rep.sort_values(by=\"max_drawdown\", ascending=True)\n",
    "\n",
    "        vol_dd_rep.to_latex(OUTDIR / \"step3_vol_dd_all.tex\", float_format=\"%.2f\")\n",
    "        print(\"Overleaf table written: outputs/step3_vol_dd_all.tex (Semis + GLD/TLT/VIXY)\")\n",
    "    else:\n",
    "        print(\"âš ï¸ Curated Overleaf table empty (no overlapping columns).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "ea4415b524268d77",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Subset: semis â†’ wrote .tex + .png to outputs/\n",
      "\n",
      "Subset: diversifiers â†’ wrote .tex + .png to outputs/\n",
      "\n",
      "Subset: factors â†’ wrote .tex + .png to outputs/\n",
      "\n",
      "Subset: all â†’ wrote .tex + .png to outputs/\n",
      "Overleaf table written: outputs/step3_vol_dd_all.tex (Semis + GLD/TLT/VIXY)\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T20:33:11.844836Z",
     "start_time": "2025-08-19T20:33:06.472033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# step4_factors.py\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_datareader.data as web\n",
    "\n",
    "OUTDIR = Path(\"outputs\"); OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "EXCESS_FILE = OUTDIR / \"step2_excess_returns.csv\"\n",
    "FACTORS_FILE = OUTDIR / \"step4_all_factors.csv\"\n",
    "\n",
    "# -------------------------\n",
    "# Helpers\n",
    "# -------------------------\n",
    "def safe_mean(df, cols, name):\n",
    "    \"\"\"Return equal-weight basket return if all columns exist, else NaN series.\"\"\"\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        print(f\"âš ï¸ Skipping {name}, missing: {missing}\")\n",
    "        return pd.Series(np.nan, index=df.index, name=name)\n",
    "    return df[cols].mean(axis=1).rename(name)\n",
    "\n",
    "# -------------------------\n",
    "# Main\n",
    "# -------------------------\n",
    "def main():\n",
    "    # Load excess returns\n",
    "    excess = pd.read_csv(EXCESS_FILE, index_col=0, parse_dates=True)\n",
    "    excess = excess.apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "    # -------------------------\n",
    "    # Custom baskets\n",
    "    # -------------------------\n",
    "    semi = safe_mean(excess, [\"NVDA\", \"AMD\", \"INTC\"], \"SemiBasket\")\n",
    "\n",
    "    hypers = safe_mean(excess, [\"MSFT\", \"AMZN\", \"GOOGL\"], \"HyperscalerBasket\")\n",
    "\n",
    "    semi_capex = safe_mean(excess, [\"AMAT\", \"ASML\", \"LRCX\", \"KLAC\"], \"SemiCapex\")\n",
    "\n",
    "    # hyperscaler capex proxy = same hyperscalers (in practice: infra spend)\n",
    "    hyper_capex = safe_mean(excess, [\"MSFT\", \"AMZN\", \"GOOGL\"], \"HyperCapex\")\n",
    "\n",
    "    semi_vs_hypers = (semi - hypers).rename(\"Semi_vs_Hypers\")\n",
    "    semi_capex_vs_hypercapex = (semi_capex - hyper_capex).rename(\"SemiCapex_vs_HyperCapex\")\n",
    "\n",
    "    custom_factors = pd.concat(\n",
    "        [semi, hypers, semi_capex, hyper_capex, semi_vs_hypers, semi_capex_vs_hypercapex],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Fama-French Factors\n",
    "    # -------------------------\n",
    "    try:\n",
    "        ff5 = web.DataReader(\"F-F_Research_Data_5_Factors_2x3_daily\", \"famafrench\")[0]\n",
    "        ff5.index = pd.to_datetime(ff5.index, format=\"%Y%m%d\")\n",
    "        ff5 = ff5.rename_axis(\"Date\") / 100.0  # convert % to decimals\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Could not download Fama-French 5-factors: {e}\")\n",
    "        ff5 = pd.DataFrame(index=excess.index)\n",
    "\n",
    "    try:\n",
    "        mom = web.DataReader(\"F-F_Momentum_Factor_daily\", \"famafrench\")[0]\n",
    "        mom.index = pd.to_datetime(mom.index, format=\"%Y%m%d\")\n",
    "        mom = mom.rename(columns={\"Mom   \": \"Mom\"}) / 100.0\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Could not download Momentum factor: {e}\")\n",
    "        mom = pd.DataFrame(index=excess.index)\n",
    "\n",
    "    # -------------------------\n",
    "    # Combine everything\n",
    "    # -------------------------\n",
    "    all_factors = pd.concat([custom_factors, ff5, mom], axis=1)\n",
    "    all_factors = all_factors.loc[excess.index].sort_index()\n",
    "    all_factors = all_factors.apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "    all_factors.to_csv(FACTORS_FILE)\n",
    "    print(f\"âœ… Saved factors to {FACTORS_FILE}\")\n",
    "    print(\"Columns:\", list(all_factors.columns))\n",
    "\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "5637fc9287f08806",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved factors to outputs/step4_all_factors.csv\n",
      "Columns: ['SemiBasket', 'HyperscalerBasket', 'SemiCapex', 'HyperCapex', 'Semi_vs_Hypers', 'SemiCapex_vs_HyperCapex', 'Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', 'RF', 'Mom']\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T20:41:14.587886Z",
     "start_time": "2025-08-19T20:33:26.683705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# step5_backtest_factors.py (fully fixed)\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import statsmodels.api as sm\n",
    "\n",
    "OUTDIR = Path(\"outputs\"); OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "EXCESS_FILE = OUTDIR / \"step2_excess_returns.csv\"\n",
    "FACTORS_FILE = OUTDIR / \"step4_all_factors.csv\"\n",
    "\n",
    "# -------------------------\n",
    "# Helpers\n",
    "# -------------------------\n",
    "def compute_perf(returns, freq=252):\n",
    "    \"\"\"Compute performance statistics for a return series.\"\"\"\n",
    "    returns = returns.dropna()\n",
    "    if returns.empty:\n",
    "        return {\"AnnRet\": np.nan, \"AnnVol\": np.nan, \"Sharpe\": np.nan, \"MaxDD\": np.nan}\n",
    "    mu = returns.mean() * freq\n",
    "    vol = returns.std() * np.sqrt(freq)\n",
    "    sharpe = mu / vol if vol > 0 else np.nan\n",
    "    cum = (1 + returns).cumprod()\n",
    "    mdd = (cum / cum.cummax() - 1).min()\n",
    "    return {\"AnnRet\": mu, \"AnnVol\": vol, \"Sharpe\": sharpe, \"MaxDD\": mdd}\n",
    "\n",
    "def beautify_plot(ax):\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%b-%y'))\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "    ax.grid(True, ls=\"--\", alpha=0.6)\n",
    "    return ax\n",
    "\n",
    "def add_line_labels(ax, spacing=5):\n",
    "    \"\"\"Put labels at end of each line, but skip if last point is NaN.\"\"\"\n",
    "    for line in ax.get_lines():\n",
    "        xdata = pd.Series(line.get_xdata())\n",
    "        ydata = pd.Series(line.get_ydata())\n",
    "        label = line.get_label()\n",
    "        mask = ydata.dropna()\n",
    "        if mask.empty:\n",
    "            continue\n",
    "        x_last, y_last = xdata.iloc[mask.index[-1]], ydata.iloc[mask.index[-1]]\n",
    "        if not np.isfinite(y_last):\n",
    "            continue\n",
    "        ax.text(\n",
    "            x_last + pd.Timedelta(days=spacing),\n",
    "            y_last,\n",
    "            label,\n",
    "            color=line.get_color(),\n",
    "            va=\"center\",\n",
    "            fontsize=8\n",
    "        )\n",
    "\n",
    "# -------------------------\n",
    "# Rolling regression\n",
    "# -------------------------\n",
    "def rolling_regression(y, x, window=252, min_obs=50):\n",
    "    betas, alphas, dates = [], [], []\n",
    "    for i in range(window, len(y)):\n",
    "        y_win = y.iloc[i-window:i]\n",
    "        x_win = x.iloc[i-window:i]\n",
    "        df = pd.concat([y_win, x_win], axis=1).dropna()\n",
    "        if len(df) < min_obs:\n",
    "            betas.append(np.nan); alphas.append(np.nan); dates.append(y.index[i]); continue\n",
    "        y_clean = df.iloc[:, 0]\n",
    "        X_clean = sm.add_constant(df.iloc[:, 1])\n",
    "        try:\n",
    "            res = sm.OLS(y_clean, X_clean, missing=\"drop\").fit()\n",
    "            betas.append(res.params.iloc[1])\n",
    "            alphas.append(res.params.iloc[0])\n",
    "        except Exception:\n",
    "            betas.append(np.nan); alphas.append(np.nan)\n",
    "        dates.append(y.index[i])\n",
    "    return pd.Series(betas, index=dates), pd.Series(alphas, index=dates)\n",
    "\n",
    "# -------------------------\n",
    "# Main\n",
    "# -------------------------\n",
    "def main():\n",
    "    excess = pd.read_csv(EXCESS_FILE, index_col=0, parse_dates=True).sort_index()\n",
    "    factors = pd.read_csv(FACTORS_FILE, index_col=0, parse_dates=True).sort_index()\n",
    "\n",
    "    # Standardize factor column names\n",
    "    factors.columns = factors.columns.str.strip()\n",
    "    if \"Mkt-RF\" not in factors.columns:\n",
    "        for col in factors.columns:\n",
    "            if col.lower().replace(\"-\", \"\").replace(\"_\", \"\") in [\"mktrf\", \"marketexcess\", \"mkt\"]:\n",
    "                factors = factors.rename(columns={col: \"Mkt-RF\"})\n",
    "\n",
    "    # --- Truncate FF factors to 30th June of last available year ---\n",
    "    cutoff = pd.Timestamp(f\"{factors.index[-1].year}-06-30\")\n",
    "    ff_cols = [\"Mkt-RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\", \"RF\", \"Mom\"]\n",
    "    for col in [c for c in ff_cols if c in factors.columns]:\n",
    "        factors.loc[factors.index > cutoff, col] = np.nan\n",
    "\n",
    "    # Align with excess returns\n",
    "    idx = excess.index.intersection(factors.index)\n",
    "    factors, excess = factors.loc[idx], excess.loc[idx]\n",
    "\n",
    "    # 1. Factor performance\n",
    "    perf_stats = {col: compute_perf(factors[col]) for col in factors.columns}\n",
    "    perf_df = pd.DataFrame(perf_stats).T\n",
    "    perf_df.to_csv(OUTDIR / \"step5_factor_performance.csv\")\n",
    "\n",
    "    # Combined cumulative returns (skip leading NaNs, no fillna(0))\n",
    "    cumrets = (1 + factors).cumprod() * 10000\n",
    "    ax = cumrets.plot(figsize=(10, 6), lw=1)\n",
    "    ax.set_title(\"Cumulative Returns of Factors\")\n",
    "    ax.set_ylabel(\"Growth of $10,000\")\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    beautify_plot(ax)\n",
    "    add_line_labels(ax)\n",
    "    ax.legend().remove()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTDIR / \"step5_cumulative_returns.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    # Individual cumulative plots\n",
    "    for fac in factors.columns:\n",
    "        series = factors[fac].copy()\n",
    "        series = series.loc[series.first_valid_index():]  # drop leading NaN\n",
    "        cumret = (1 + series).cumprod() * 10000\n",
    "\n",
    "        ax = cumret.plot(figsize=(8, 4), lw=1)\n",
    "        ax.set_title(f\"Cumulative Return: {fac}\")\n",
    "        ax.set_ylabel(\"Growth of $10,000\")\n",
    "        ax.set_xlabel(\"Date\")\n",
    "        beautify_plot(ax)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(OUTDIR / f\"step5_cumulative_{fac}.png\", dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "    # -------------------------\n",
    "    # Rolling regressions: all assets vs each factor\n",
    "    # -------------------------\n",
    "    window = 252\n",
    "    all_betas, all_alphas = {}, {}\n",
    "\n",
    "    for asset in excess.columns:\n",
    "        y = excess[asset]\n",
    "        for fac in [f for f in factors.columns if f != \"RF\"]:\n",
    "            beta, alpha = rolling_regression(y, factors[[fac]], window=window)\n",
    "            all_betas[(asset, fac)] = beta\n",
    "            all_alphas[(asset, fac)] = alpha\n",
    "\n",
    "            # Plot betas\n",
    "            if not beta.dropna().empty:\n",
    "                ax = beta.plot(figsize=(8,4), lw=1)\n",
    "                ax.set_title(f\"Rolling {window}d Beta: {asset} vs {fac}\")\n",
    "                ax.set_ylabel(\"Beta\")\n",
    "                beautify_plot(ax)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(OUTDIR / f\"step5_beta_{asset}_{fac}.png\", dpi=150)\n",
    "                plt.close()\n",
    "\n",
    "            # Plot alphas\n",
    "            if not alpha.dropna().empty:\n",
    "                ax = alpha.plot(figsize=(8,4), lw=1, color=\"purple\")\n",
    "                ax.set_title(f\"Rolling {window}d Alpha: {asset} vs {fac}\")\n",
    "                ax.set_ylabel(\"Alpha\")\n",
    "                beautify_plot(ax)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(OUTDIR / f\"step5_alpha_{asset}_{fac}.png\", dpi=150)\n",
    "                plt.close()\n",
    "\n",
    "    # Save rolling betas/alphas to wide-format CSVs\n",
    "    betas_df = pd.DataFrame(all_betas)\n",
    "    alphas_df = pd.DataFrame(all_alphas)\n",
    "    betas_df.to_csv(OUTDIR / \"step5_rolling_betas.csv\")\n",
    "    alphas_df.to_csv(OUTDIR / \"step5_rolling_alphas.csv\")\n",
    "\n",
    "    # Factor summary table (no per-asset betas here, just factor stats)\n",
    "    final_growth = (1 + factors).cumprod() * 10000\n",
    "    summary = pd.DataFrame({\n",
    "        \"FinalValue($10k)\": final_growth.iloc[-1].round(2),\n",
    "        \"AnnRet\": perf_df[\"AnnRet\"],\n",
    "        \"Sharpe\": perf_df[\"Sharpe\"]\n",
    "    })\n",
    "    summary.to_csv(OUTDIR / \"step5_factor_summary.csv\")\n",
    "    summary.round(3).to_latex(OUTDIR / \"step5_factor_summary.tex\")\n",
    "\n",
    "    print(\"\\nâœ“ Step 5 complete: factor performance, cumulative plots, rolling regressions (all assets), summary table.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "d8bb47692fb2680b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Step 5 complete: factor performance, cumulative plots, rolling regressions (all assets), summary table.\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T15:43:13.518550Z",
     "start_time": "2025-08-19T15:42:54.432892Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# step5c_backtest_portfolio.py\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import statsmodels.api as sm\n",
    "\n",
    "OUTDIR = Path(\"outputs\"); OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "EXCESS_FILE = OUTDIR / \"step2_excess_returns.csv\"\n",
    "FACTORS_FILE = OUTDIR / \"step4_all_factors.csv\"\n",
    "\n",
    "# ---- Config ----\n",
    "SEMIS = [\"NVDA\",\"AMD\",\"ASML\",\"AMAT\",\"TSM\",\"LRCX\",\"INTC\"]\n",
    "DIVERSIFIERS = [\"GLD\",\"TLT\",\"VIXY\"]\n",
    "NAMES = SEMIS + DIVERSIFIERS\n",
    "BENCHMARK = \"SMH\"\n",
    "\n",
    "# ---- Hard-coded Market Caps (USD) ----\n",
    "MKT_CAPS = {\n",
    "    \"ASML\": 295.706e9,\n",
    "    \"TSM\": 1023.605e9,\n",
    "    \"AMAT\": 131.610e9,\n",
    "    \"AMD\": 282.294e9,\n",
    "    \"INTC\": 109.556e9,\n",
    "    \"LRCX\": 125.309e9,\n",
    "    \"NVDA\": 4449.584e9,\n",
    "    # scaled for diversifiers for meaningful presence\n",
    "    \"GLD\": 250e9,\n",
    "    \"TLT\": 500e9,\n",
    "    \"VIXY\": 50e9,\n",
    "}\n",
    "\n",
    "# -------------------------\n",
    "# Helpers\n",
    "# -------------------------\n",
    "def compute_perf(returns, freq=252):\n",
    "    \"\"\"Performance metrics for a return series.\"\"\"\n",
    "    r = returns.dropna()\n",
    "    if r.empty:\n",
    "        return {\"AnnRet\": np.nan, \"AnnVol\": np.nan, \"Sharpe\": np.nan, \"MaxDD\": np.nan}\n",
    "    mu = r.mean() * freq\n",
    "    vol = r.std() * np.sqrt(freq)\n",
    "    sharpe = mu / vol if vol > 0 else np.nan\n",
    "    cum = (1 + r).cumprod()\n",
    "    mdd = (cum / cum.cummax() - 1).min()\n",
    "    return {\"AnnRet\": mu, \"AnnVol\": vol, \"Sharpe\": sharpe, \"MaxDD\": mdd}\n",
    "\n",
    "def beautify(ax):\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%b-%y\"))\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "    ax.grid(True, ls=\"--\", alpha=0.6)\n",
    "    return ax\n",
    "\n",
    "def ols_hac_table(y, X, nw_lags=20):\n",
    "    \"\"\"OLS with HAC (Neweyâ€“West) SEs; returns a tidy table.\"\"\"\n",
    "    df = pd.concat([y, X], axis=1).dropna()\n",
    "    if df.empty or df.shape[1] < 2:\n",
    "        return pd.DataFrame()\n",
    "    y_clean = df.iloc[:, 0]\n",
    "    X_clean = sm.add_constant(df.iloc[:, 1:])\n",
    "    res = sm.OLS(y_clean, X_clean).fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": nw_lags})\n",
    "    out = pd.DataFrame({\"coef\": res.params, \"tstat\": res.tvalues, \"pval\": res.pvalues})\n",
    "    out.loc[\"R2\", [\"coef\",\"tstat\",\"pval\"]] = [res.rsquared, np.nan, np.nan]\n",
    "    # pretty p-values for LaTeX\n",
    "    def fmt_p(v):\n",
    "        try:\n",
    "            if np.isfinite(v):\n",
    "                return \"<0.001\" if v < 1e-3 else f\"{v:.3f}\"\n",
    "        except Exception:\n",
    "            pass\n",
    "        return \"\"\n",
    "    out[\"pval\"] = out[\"pval\"].apply(fmt_p)\n",
    "    return out\n",
    "\n",
    "def rolling_regression(y, x, window=252, min_obs=50):\n",
    "    \"\"\"Rolling regression of y on a single factor x (DataFrame with one column).\"\"\"\n",
    "    betas, alphas, dates = [], [], []\n",
    "    for i in range(window, len(y)):\n",
    "        df = pd.concat([y.iloc[i-window:i], x.iloc[i-window:i]], axis=1).dropna()\n",
    "        dates.append(y.index[i])\n",
    "        if len(df) < min_obs:\n",
    "            betas.append(np.nan); alphas.append(np.nan); continue\n",
    "        res = sm.OLS(df.iloc[:,0], sm.add_constant(df.iloc[:,1])).fit()\n",
    "        betas.append(res.params.iloc[1])\n",
    "        alphas.append(res.params.iloc[0])\n",
    "    return pd.Series(betas, index=dates), pd.Series(alphas, index=dates)\n",
    "\n",
    "# -------------------------\n",
    "# Portfolio Constructors\n",
    "# -------------------------\n",
    "def build_equal_weight(excess):\n",
    "    names_present = [c for c in NAMES if c in excess.columns]\n",
    "    missing = sorted(set(NAMES) - set(names_present))\n",
    "    if missing:\n",
    "        print(f\"âš ï¸  Missing in returns (EW): {missing}\")\n",
    "    if not names_present:\n",
    "        raise ValueError(\"No portfolio names found in excess returns.\")\n",
    "    return excess[names_present].mean(axis=1).rename(\"Portfolio_EW\")\n",
    "\n",
    "def build_mcap_weight(excess):\n",
    "    names_present = [c for c in NAMES if c in excess.columns]\n",
    "    missing = sorted(set(NAMES) - set(names_present))\n",
    "    if missing:\n",
    "        print(f\"âš ï¸  Missing in returns (MCAP): {missing}\")\n",
    "\n",
    "    weights = pd.Series(MKT_CAPS).reindex(names_present).dropna()\n",
    "    weights = weights / weights.sum()\n",
    "\n",
    "    print(\"\\nðŸ“Š Hard-coded Market-cap weights (normalised):\")\n",
    "    print(weights.round(3))\n",
    "\n",
    "    # Save CSV and LaTeX for Overleaf\n",
    "    OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "    weights.to_csv(OUTDIR / \"step5_mcap_weights.csv\")\n",
    "\n",
    "    wdf = weights.rename(\"Weight\").to_frame()\n",
    "    wdf.index.name = \"Ticker\"\n",
    "    wdf.loc[\"Total\", \"Weight\"] = wdf[\"Weight\"].sum()\n",
    "    wdf.to_latex(OUTDIR / \"step5_mcap_weights.tex\",\n",
    "                 float_format=lambda x: f\"{x:.3f}\",\n",
    "                 index=True, escape=False)\n",
    "\n",
    "    return (excess[names_present] @ weights).rename(\"Portfolio_MCAP\")\n",
    "\n",
    "# -------------------------\n",
    "# Main\n",
    "# -------------------------\n",
    "def main():\n",
    "    # Load data\n",
    "    excess = pd.read_csv(EXCESS_FILE, index_col=0, parse_dates=True).sort_index()\n",
    "    factors = pd.read_csv(FACTORS_FILE, index_col=0, parse_dates=True).sort_index()\n",
    "\n",
    "    # Align dates\n",
    "    idx = excess.index.intersection(factors.index)\n",
    "    excess, factors = excess.loc[idx], factors.loc[idx]\n",
    "\n",
    "    # Benchmark\n",
    "    if BENCHMARK not in excess.columns:\n",
    "        raise KeyError(f\"Benchmark {BENCHMARK} not found in excess returns columns.\")\n",
    "    bench = excess[BENCHMARK].rename(\"Benchmark\")\n",
    "\n",
    "    # Build portfolios\n",
    "    ew = build_equal_weight(excess)\n",
    "    mcap = build_mcap_weight(excess)\n",
    "    portfolios = pd.concat([ew, mcap, bench], axis=1)\n",
    "\n",
    "    # --- Performance comparison ---\n",
    "    perf = pd.DataFrame({col: compute_perf(portfolios[col]) for col in portfolios.columns}).T\n",
    "    perf.to_latex(OUTDIR / \"step5_perf_portfolios_vs_smh.tex\", float_format=\"%.3f\", escape=True)\n",
    "\n",
    "    # --- Cumulative returns plot ---\n",
    "    cum = (1 + portfolios).cumprod() * 10000\n",
    "    ax = cum.plot(figsize=(9, 6))\n",
    "    ax.set_title(\"Portfolio (EW & MCAP) vs SMH\")\n",
    "    ax.set_ylabel(\"Growth of $10,000\")\n",
    "    beautify(ax)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTDIR / \"step5_portfolios_vs_smh.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    # --- Static regressions (HAC / Neweyâ€“West) ---\n",
    "    # Normalise factor column names (handle both Mkt_RF and Mkt-RF)\n",
    "    factors.columns = [c.strip().replace(\"Mkt_RF\", \"Mkt-RF\") for c in factors.columns]\n",
    "\n",
    "    NW_LAGS = 20  # ~1 trading month (tweakable)\n",
    "    # Two specs: main \"levels\" for paper; \"spread\" optional for appendix\n",
    "    LEVELS = [\"Mkt-RF\",\"SMB\",\"HML\",\"RMW\",\"CMA\",\"Mom\",\"SemiCapex\"]\n",
    "    SPREAD = [\"Mkt-RF\",\"SMB\",\"HML\",\"RMW\",\"CMA\",\"Mom\",\"Semi vs Hypers\"]\n",
    "\n",
    "    def safe_pick(cols):\n",
    "        return [c for c in cols if c in factors.columns]\n",
    "\n",
    "    for col in [\"Portfolio_EW\", \"Portfolio_MCAP\"]:\n",
    "        # MAIN tables (levels) to canonical filenames used by your LaTeX\n",
    "        X_levels = factors.reindex(columns=safe_pick(LEVELS))\n",
    "        tbl_levels = ols_hac_table(portfolios[col], X_levels, nw_lags=NW_LAGS)\n",
    "        if not tbl_levels.empty:\n",
    "            tbl_levels.to_latex(OUTDIR / f\"step5_regressions_{col}.tex\",\n",
    "                                float_format=\"%.3f\", escape=False)\n",
    "\n",
    "        # APPENDIX tables (spread) to side files (optional to include)\n",
    "        X_spread = factors.reindex(columns=safe_pick(SPREAD))\n",
    "        tbl_spread = ols_hac_table(portfolios[col], X_spread, nw_lags=NW_LAGS)\n",
    "        if not tbl_spread.empty:\n",
    "            tbl_spread.to_latex(OUTDIR / f\"step5_regressions_{col}_spread.tex\",\n",
    "                                float_format=\"%.3f\", escape=False)\n",
    "\n",
    "    # --- Rolling regressions ---\n",
    "    fac_list = [c for c in factors.columns if c != \"RF\"]\n",
    "    for col in [\"Portfolio_EW\", \"Portfolio_MCAP\"]:\n",
    "        for fac in fac_list:\n",
    "            beta, alpha = rolling_regression(portfolios[col], factors[[fac]])\n",
    "            if not beta.dropna().empty:\n",
    "                ax = beta.plot(figsize=(8, 4))\n",
    "                ax.set_title(f\"Rolling Beta: {col} vs {fac}\")\n",
    "                ax.set_ylabel(\"Beta\")\n",
    "                beautify(ax)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(OUTDIR / f\"step5_rolling_beta_{col}_{fac}.png\", dpi=150)\n",
    "                plt.close()\n",
    "            if not alpha.dropna().empty:\n",
    "                ax = alpha.plot(figsize=(8, 4))\n",
    "                ax.set_title(f\"Rolling Alpha: {col} vs {fac}\")\n",
    "                ax.set_ylabel(\"Alpha\")\n",
    "                beautify(ax)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(OUTDIR / f\"step5_rolling_alpha_{col}_{fac}.png\", dpi=150)\n",
    "                plt.close()\n",
    "\n",
    "    print(\"âœ“ Step 5c complete: EW & MCAP portfolios vs SMH, HAC regressions, rolling betas/alphas, and LaTeX tables/images written.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "b8a9ae61309cecdc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Hard-coded Market-cap weights (normalised):\n",
      "NVDA    0.616\n",
      "AMD     0.039\n",
      "ASML    0.041\n",
      "AMAT    0.018\n",
      "TSM     0.142\n",
      "LRCX    0.017\n",
      "INTC    0.015\n",
      "GLD     0.035\n",
      "TLT     0.069\n",
      "VIXY    0.007\n",
      "dtype: float64\n",
      "âœ“ Step 5c complete: EW & MCAP portfolios vs SMH, HAC regressions, rolling betas/alphas, and LaTeX tables/images written.\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T16:16:13.270604Z",
     "start_time": "2025-08-19T16:16:12.498741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# step5d_risk_corr.py (diagnostics: block medians, ex-VIXY check, long-form pairs, data QA)\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "OUTDIR = Path(\"outputs\"); OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "EXCESS_FILE = OUTDIR / \"step2_excess_returns.csv\"\n",
    "\n",
    "# ---- Config ----\n",
    "SEMIS = [\"NVDA\",\"AMD\",\"ASML\",\"AMAT\",\"TSM\",\"LRCX\",\"INTC\"]\n",
    "DIVERSIFIERS = [\"GLD\",\"TLT\",\"VIXY\"]\n",
    "BASE = SEMIS\n",
    "ALL = SEMIS + DIVERSIFIERS\n",
    "BENCHMARK = \"SMH\"   # display only\n",
    "\n",
    "# Reference weights (MCAP-style) for risk contributions (on ALL)\n",
    "FINAL_WEIGHTS = {\n",
    "    \"ASML\": 0.041,\n",
    "    \"TSM\":  0.142,\n",
    "    \"AMAT\": 0.018,\n",
    "    \"AMD\":  0.039,\n",
    "    \"INTC\": 0.015,\n",
    "    \"LRCX\": 0.017,\n",
    "    \"NVDA\": 0.616,\n",
    "    \"GLD\":  0.035,\n",
    "    \"TLT\":  0.069,\n",
    "    \"VIXY\": 0.007,\n",
    "}\n",
    "\n",
    "# -------------------------\n",
    "# Helpers\n",
    "# -------------------------\n",
    "def ensure_cols(df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
    "    present = [c for c in cols if c in df.columns]\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        print(f\"âš ï¸ Missing columns skipped: {missing}\")\n",
    "    return df[present]\n",
    "\n",
    "def ann_vol(series: pd.Series, freq: int = 252) -> float:\n",
    "    return float(series.std() * np.sqrt(freq))\n",
    "\n",
    "def offdiag_median_corr(df: pd.DataFrame) -> float:\n",
    "    C = df.corr()\n",
    "    n = C.shape[0]\n",
    "    if n <= 1:\n",
    "        return np.nan\n",
    "    mask = ~np.eye(n, dtype=bool)\n",
    "    return float(C.where(mask).stack().median())\n",
    "\n",
    "def save_corr_heatmap(df: pd.DataFrame, fname_png: Path, title: str):\n",
    "    corr = df.corr()\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    im = ax.imshow(corr.values, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "    ax.set_xticks(range(len(corr.columns)))\n",
    "    ax.set_yticks(range(len(corr.columns)))\n",
    "    ax.set_xticklabels(corr.columns, rotation=45, ha=\"right\")\n",
    "    ax.set_yticklabels(corr.columns)\n",
    "    fig.colorbar(im, ax=ax)\n",
    "    ax.set_title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fname_png, dpi=150)\n",
    "    plt.close()\n",
    "    return corr\n",
    "\n",
    "def risk_contrib_table(returns: pd.DataFrame, weights: dict[str, float]) -> pd.DataFrame:\n",
    "    cov = returns.cov() * 252.0\n",
    "    w = pd.Series(weights, dtype=float).reindex(cov.columns).fillna(0.0)\n",
    "    sigma_p = float(np.sqrt(np.dot(w, np.dot(cov.values, w))))\n",
    "    if sigma_p == 0:\n",
    "        mrc = pd.Series(0.0, index=cov.columns)\n",
    "        prc = pd.Series(0.0, index=cov.columns)\n",
    "    else:\n",
    "        mrc = pd.Series(np.dot(cov.values, w) / sigma_p, index=cov.columns)\n",
    "        prc = (w * mrc) / sigma_p\n",
    "    out = pd.DataFrame({\"Weight\": w, \"MRC\": mrc, \"PRC\": prc}, index=cov.columns)\n",
    "    out = out.loc[out[\"Weight\"].sort_values(ascending=False).index]\n",
    "    return out\n",
    "\n",
    "def write_vols_table(returns: pd.DataFrame, fname_tex: Path, median_offdiag: float):\n",
    "    vols = returns.apply(ann_vol, axis=0).rename(\"AnnVol\").to_frame()\n",
    "    vols.index.name = \"Ticker\"\n",
    "    summary_row = pd.DataFrame({\"AnnVol\": [median_offdiag]}, index=[\"Median off-diag corr\"])\n",
    "    table = pd.concat([vols, summary_row])\n",
    "    table.to_latex(fname_tex, float_format=\"%.3f\", escape=True)\n",
    "\n",
    "def pairwise_longform(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    C = df.corr()\n",
    "    tri = C.where(~np.tril(np.ones(C.shape, dtype=bool)))  # keep upper triangle off-diag\n",
    "    long = tri.stack().reset_index()\n",
    "    long.columns = [\"A\", \"B\", \"corr\"]\n",
    "    return long\n",
    "\n",
    "def block_medians(long_pairs: pd.DataFrame, semis: set, divers: set) -> dict:\n",
    "    def tag(a, b):\n",
    "        if a in semis and b in semis:\n",
    "            return \"semi-semi\"\n",
    "        if (a in semis and b in divers) or (a in divers and b in semis):\n",
    "            return \"semi-div\"\n",
    "        if a in divers and b in divers:\n",
    "            return \"div-div\"\n",
    "        return \"other\"\n",
    "    t = long_pairs.copy()\n",
    "    t[\"block\"] = [tag(a, b) for a, b in zip(t[\"A\"], t[\"B\"])]\n",
    "    out = {}\n",
    "    for blk in [\"semi-semi\", \"semi-div\", \"div-div\", \"other\"]:\n",
    "        s = t.loc[t[\"block\"] == blk, \"corr\"]\n",
    "        if not s.empty:\n",
    "            out[blk] = float(s.median())\n",
    "    out[\"n_pairs_total\"] = int(len(t))\n",
    "    out[\"n_pairs_by_block\"] = {blk: int((t[\"block\"] == blk).sum()) for blk in [\"semi-semi\",\"semi-div\",\"div-div\",\"other\"]}\n",
    "    return out\n",
    "\n",
    "def data_quality_checks(df: pd.DataFrame, name: str):\n",
    "    # duplicate dates\n",
    "    if df.index.duplicated().any():\n",
    "        dup_n = int(df.index.duplicated().sum())\n",
    "        print(f\"âš ï¸ {name}: dropped {dup_n} duplicated dates.\")\n",
    "        df = df[~df.index.duplicated(keep=\"first\")]\n",
    "    # constant/near-constant series\n",
    "    sds = df.std()\n",
    "    const_cols = sds.index[sds < 1e-12].tolist()\n",
    "    if const_cols:\n",
    "        print(f\"âš ï¸ {name}: near-constant series detected (std<1e-12): {const_cols}\")\n",
    "    return df\n",
    "\n",
    "# -------------------------\n",
    "# Main\n",
    "# -------------------------\n",
    "def main():\n",
    "    # Load returns (fresh each run)\n",
    "    excess = pd.read_csv(EXCESS_FILE, index_col=0, parse_dates=True).sort_index()\n",
    "    excess = data_quality_checks(excess, \"excess\")\n",
    "\n",
    "    # Build aligned window on ALL (semis + diversifiers)\n",
    "    all_df = ensure_cols(excess, ALL).dropna(how=\"any\")\n",
    "    if all_df.empty:\n",
    "        raise ValueError(\"No overlapping data for ALL in step2_excess_returns.csv\")\n",
    "\n",
    "    # Base = semis (same dates as ALL)\n",
    "    base_df = all_df[BASE]\n",
    "\n",
    "    # With benchmark (same dates)\n",
    "    with_bench = all_df.join(ensure_cols(excess, [BENCHMARK]), how=\"inner\").dropna(how=\"any\")\n",
    "\n",
    "    # Diagnostics: spans\n",
    "    def span(df):\n",
    "        return (df.index.min().date(), df.index.max().date(), df.shape[0], df.shape[1])\n",
    "    b0, b1, bn, bk = span(base_df)\n",
    "    a0, a1, an, ak = span(all_df)\n",
    "    print(f\"BASE (semis) window: {b0} â†’ {b1}, n={bn} days, k={bk} assets\")\n",
    "    print(f\"ALL  (semis+div)   : {a0} â†’ {a1}, n={an} days, k={ak} assets\")\n",
    "    if BENCHMARK in with_bench.columns:\n",
    "        w0, w1, wn, wk = span(with_bench)\n",
    "        print(f\"WITH_BENCH (+{BENCHMARK}): {w0} â†’ {w1}, n={wn} days, k={wk} assets\")\n",
    "\n",
    "    # === Correlations / vols ===\n",
    "    corr_base = save_corr_heatmap(base_df, OUTDIR / \"step5d_corr_base.png\",\n",
    "                                  \"Correlation Matrix (BASE = Semis)\")\n",
    "    corr_base.to_csv(OUTDIR / \"step5d_corr_base.csv\")\n",
    "    med_base = offdiag_median_corr(base_df)\n",
    "    (OUTDIR / \"step5d_median_corr_base.tex\").write_text(f\"{med_base:.2f}\")\n",
    "    write_vols_table(base_df, OUTDIR / \"step5d_vols_corrs_base.tex\", med_base)\n",
    "\n",
    "    corr_all = save_corr_heatmap(all_df, OUTDIR / \"step5d_corr_all.png\",\n",
    "                                 \"Correlation Matrix (ALL = Semis + Diversifiers)\")\n",
    "    corr_all.to_csv(OUTDIR / \"step5d_corr_all.csv\")\n",
    "    med_all = offdiag_median_corr(all_df)\n",
    "    (OUTDIR / \"step5d_median_corr_all.tex\").write_text(f\"{med_all:.2f}\")\n",
    "    write_vols_table(all_df, OUTDIR / \"step5d_vols_corrs_all.tex\", med_all)\n",
    "\n",
    "    # Optional third figure including SMH for reference\n",
    "    if BENCHMARK in with_bench.columns:\n",
    "        corr_bench = save_corr_heatmap(with_bench, OUTDIR / \"step5d_corr_with_bench.png\",\n",
    "                                       f\"Correlation Matrix (Semis + Diversifiers + {BENCHMARK})\")\n",
    "        corr_bench.to_csv(OUTDIR / \"step5d_corr_with_bench.csv\")\n",
    "        med_bench = offdiag_median_corr(with_bench)\n",
    "        (OUTDIR / \"step5d_median_corr_with_bench.tex\").write_text(f\"{med_bench:.2f}\")\n",
    "\n",
    "    # === New diagnostics: block medians & long-form pairs ===\n",
    "    semis_set, divers_set = set(SEMIS), set(DIVERSIFIERS)\n",
    "\n",
    "    base_long = pairwise_longform(base_df)\n",
    "    all_long  = pairwise_longform(all_df)\n",
    "\n",
    "    base_blocks = block_medians(base_long, semis_set, divers_set)\n",
    "    all_blocks  = block_medians(all_long,  semis_set, divers_set)\n",
    "\n",
    "    # Save long-form pair tables\n",
    "    base_long.to_csv(OUTDIR / \"step5d_pairwise_corrs_base.csv\", index=False)\n",
    "    all_long.to_csv(OUTDIR  / \"step5d_pairwise_corrs_all.csv\", index=False)\n",
    "\n",
    "    # Print block medians\n",
    "    def pblock(tag, d):\n",
    "        n = d.get(\"n_pairs_total\")\n",
    "        nb = d.get(\"n_pairs_by_block\", {})\n",
    "        print(f\"{tag} pairs total: {n}, by block: {nb}\")\n",
    "        for k in [\"semi-semi\",\"semi-div\",\"div-div\"]:\n",
    "            if k in d:\n",
    "                print(f\"  median {k}: {d[k]:.2f}\")\n",
    "\n",
    "    pblock(\"BASE\", base_blocks)\n",
    "    pblock(\"ALL \", all_blocks)\n",
    "\n",
    "    # Ex-VIXY sensitivity (how much VIXY pulls the median)\n",
    "    if \"VIXY\" in all_df.columns:\n",
    "        all_no_vixy = all_df.drop(columns=[\"VIXY\"])\n",
    "        med_all_no_vixy = offdiag_median_corr(all_no_vixy)\n",
    "        print(f\"Median off-diagonal correlation (ALL \\\\ VIXY): {med_all_no_vixy:.2f}\")\n",
    "        (OUTDIR / \"step5d_median_corr_all_exVIXY.tex\").write_text(f\"{med_all_no_vixy:.2f}\")\n",
    "\n",
    "    print(f\"Median off-diagonal correlation (BASE=Semis): {med_base:.2f}\")\n",
    "    print(f\"Median off-diagonal correlation (ALL =Semis+Diversifiers): {med_all:.2f}\")\n",
    "\n",
    "    # === Risk contributions on INVESTABLE set only (ALL) ===\n",
    "    # Equal Weight on ALL\n",
    "    ew_weights = {t: 1.0 / len(all_df.columns) for t in all_df.columns}\n",
    "    rc_ew = risk_contrib_table(all_df, ew_weights)\n",
    "    rc_ew.to_csv(OUTDIR / \"step5d_risk_contrib_EW.csv\")\n",
    "    rc_ew.to_latex(OUTDIR / \"step5d_risk_contrib_EW.tex\", float_format=\"%.3f\", escape=True)\n",
    "\n",
    "    # MCAP-like reference weights (normalize to 1 across ALL names)\n",
    "    w_ref = pd.Series(FINAL_WEIGHTS, dtype=float).reindex(all_df.columns).fillna(0.0)\n",
    "    if w_ref.sum() == 0:\n",
    "        raise ValueError(\"FINAL_WEIGHTS do not overlap with ALL; please check tickers.\")\n",
    "    w_ref = (w_ref / w_ref.sum()).to_dict()\n",
    "    rc_mcap = risk_contrib_table(all_df, w_ref)\n",
    "    rc_mcap.to_csv(OUTDIR / \"step5d_risk_contrib_MCAP.csv\")\n",
    "    rc_mcap.to_latex(OUTDIR / \"step5d_risk_contrib_MCAP.tex\", float_format=\"%.3f\", escape=True)\n",
    "\n",
    "    # Overleaf â€œone tableâ€\n",
    "    rc_mcap.to_latex(OUTDIR / \"step5d_risk_contrib_all.tex\", float_format=\"%.3f\", escape=True)\n",
    "\n",
    "    print(\"âœ“ Step 5d complete: vols, off-diag medians, heatmaps, risk contribs, and diagnostics written.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "678bb0d2db1ccbc1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE (semis) window: 2022-08-23 â†’ 2025-08-18, n=749 days, k=7 assets\n",
      "ALL  (semis+div)   : 2022-08-23 â†’ 2025-08-18, n=749 days, k=10 assets\n",
      "WITH_BENCH (+SMH): 2022-08-23 â†’ 2025-08-18, n=749 days, k=11 assets\n",
      "BASE pairs total: 21, by block: {'semi-semi': 21, 'semi-div': 0, 'div-div': 0, 'other': 0}\n",
      "  median semi-semi: 0.67\n",
      "ALL  pairs total: 45, by block: {'semi-semi': 21, 'semi-div': 21, 'div-div': 3, 'other': 0}\n",
      "  median semi-semi: 0.67\n",
      "  median semi-div: 0.05\n",
      "  median div-div: -0.01\n",
      "Median off-diagonal correlation (ALL \\ VIXY): 0.46\n",
      "Median off-diagonal correlation (BASE=Semis): 0.67\n",
      "Median off-diagonal correlation (ALL =Semis+Diversifiers): 0.17\n",
      "âœ“ Step 5d complete: vols, off-diag medians, heatmaps, risk contribs, and diagnostics written.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T18:59:31.581286Z",
     "start_time": "2025-08-19T18:59:31.424203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Step 6 â€” Cell 1: Setup, sample Î¼/Î£, MCAP benchmark, Î´, and equilibrium Ï€ ===\n",
    "\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --------------------\n",
    "# Config\n",
    "# --------------------\n",
    "OUTDIR = Path(\"outputs\"); OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "RET_FILE = OUTDIR / \"step2_excess_returns.csv\"      # daily EXCESS returns (already net of RF)\n",
    "MCAP_WEIGHTS_CSV = OUTDIR / \"step5_mcap_weights.csv\"  # optional benchmark weights from Step 5\n",
    "\n",
    "FREQ = 252\n",
    "CUTOFF = None                 # e.g., \"2025-06-30\" to freeze estimation window\n",
    "SEMIS = [\"NVDA\",\"AMD\",\"ASML\",\"AMAT\",\"TSM\",\"LRCX\",\"INTC\"]\n",
    "DIVERSIFIERS = [\"GLD\",\"TLT\",\"VIXY\"]\n",
    "ASSET_ORDER = SEMIS + DIVERSIFIERS\n",
    "\n",
    "# Fallback MCAPs (bn) if step5 CSV is absent; diversifier sleeve set below\n",
    "FALLBACK_MCAP = {\n",
    "    \"NVDA\": 3000.0, \"TSM\": 1000.0, \"ASML\": 300.0, \"AMD\": 280.0,\n",
    "    \"AMAT\": 130.0, \"LRCX\": 120.0, \"INTC\": 150.0,\n",
    "}\n",
    "DIVERSIFIER_SLEEVE = {\"GLD\": 0.06, \"TLT\": 0.06, \"VIXY\": 0.03}  # total â‰ˆ 15%\n",
    "\n",
    "# --------------------\n",
    "# Load returns & select universe\n",
    "# --------------------\n",
    "if not RET_FILE.exists():\n",
    "    raise FileNotFoundError(f\"Missing {RET_FILE}. Run Step 2 first.\")\n",
    "\n",
    "ret = pd.read_csv(RET_FILE, index_col=0, parse_dates=True)\n",
    "if CUTOFF is not None:\n",
    "    ret = ret.loc[:CUTOFF]\n",
    "\n",
    "# keep the 10 assets that are actually present\n",
    "assets = [a for a in ASSET_ORDER if a in ret.columns]\n",
    "if len(assets) < 5:\n",
    "    raise ValueError(f\"Too few of the target assets found. Present: {assets}\")\n",
    "\n",
    "R = ret[assets].copy()\n",
    "\n",
    "# --------------------\n",
    "# Annualised sample Î¼, Î£\n",
    "# --------------------\n",
    "mu_sample = R.mean() * FREQ\n",
    "Sigma_sample = R.cov() * FREQ\n",
    "\n",
    "# --------------------\n",
    "# Build MCAP benchmark weights (CSV preferred; fallback otherwise)\n",
    "# --------------------\n",
    "def build_benchmark_weights(assets, returns):\n",
    "    if MCAP_WEIGHTS_CSV.exists():\n",
    "        try:\n",
    "            w = pd.read_csv(MCAP_WEIGHTS_CSV, index_col=0).iloc[:,0]\n",
    "            w = w.reindex(assets).fillna(0.0)\n",
    "            s = w.sum()\n",
    "            if s <= 0:\n",
    "                warnings.warn(\"MCAP CSV sum <= 0. Falling back to embedded MCAP logic.\")\n",
    "            else:\n",
    "                return w / s\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Failed to read MCAP weights CSV ({e}). Falling back.\")\n",
    "    # Fallback: pro-rata semis by FALLBACK_MCAP + modest diversifier sleeve\n",
    "    w = pd.Series(0.0, index=assets, dtype=float)\n",
    "    semi_names = [a for a in assets if a in FALLBACK_MCAP]\n",
    "    sleeve = sum(DIVERSIFIER_SLEEVE.get(a,0.0) for a in assets)\n",
    "    sleeve = min(sleeve, 0.20)  # safety cap\n",
    "\n",
    "    if semi_names:\n",
    "        semi_total = 1.0 - sleeve\n",
    "        semi_sum = sum(FALLBACK_MCAP[s] for s in semi_names)\n",
    "        for s in semi_names:\n",
    "            w.loc[s] = semi_total * (FALLBACK_MCAP[s] / semi_sum)\n",
    "    for a in assets:\n",
    "        if a in DIVERSIFIER_SLEEVE:\n",
    "            w.loc[a] = DIVERSIFIER_SLEEVE[a]\n",
    "    # normalize\n",
    "    s = w.sum()\n",
    "    return w if s == 1.0 else (w / s if s > 0 else pd.Series(1/len(assets), index=assets))\n",
    "\n",
    "w_mcap = build_benchmark_weights(assets, R)\n",
    "\n",
    "# --------------------\n",
    "# Compute Î´ (Market) and equilibrium Ï€\n",
    "# --------------------\n",
    "mu_bench = float(mu_sample.loc[assets] @ w_mcap.values)       # annual excess\n",
    "sigma2_bench = float(w_mcap.values @ Sigma_sample.loc[assets, assets].values @ w_mcap.values)\n",
    "\n",
    "delta_market = (mu_bench) / sigma2_bench if sigma2_bench > 0 else 10.0  # RF already netted out in excess returns\n",
    "deltas = pd.Series({\"Trustee\": 2.0*delta_market, \"Market\": delta_market, \"Kelly\": 0.5*delta_market})\n",
    "\n",
    "# Equilibrium excess returns: Ï€ = Î´_Market * Î£ * w_mcap\n",
    "pi_sample = Sigma_sample.loc[assets, assets] @ w_mcap.loc[assets] * deltas[\"Market\"]\n",
    "pi_sample.name = \"pi\"\n",
    "\n",
    "# --------------------\n",
    "# Write Overleaf artifacts & print quick sanity\n",
    "# --------------------\n",
    "(OUTDIR / \"step6_delta_levels.tex\").write_text(deltas.to_frame(\"delta\").to_latex(escape=False, float_format=lambda x: f\"{x: .6f}\"))\n",
    "(OUTDIR / \"step6_pi_equilibrium_sample.tex\").write_text(pi_sample.to_frame(\"Value\").to_latex(escape=False, float_format=lambda x: f\"{x: .6f}\"))\n",
    "\n",
    "print(\"Universe:\", assets)\n",
    "print(\"\\nBenchmark weights (first few):\")\n",
    "print((w_mcap*100).round(2).astype(str).head(10) + \"%\")\n",
    "\n",
    "print(f\"\\nÎ´ (Market): {delta_market:.6f} | Î¼_bench: {mu_bench:.4f} | Ïƒ_bench: {np.sqrt(sigma2_bench):.4f}\")\n",
    "print(\"\\nÏ€ (sample) â€” head:\")\n",
    "print(pi_sample.round(6).head())\n"
   ],
   "id": "a39110fd85f8c248",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Universe: ['NVDA', 'AMD', 'ASML', 'AMAT', 'TSM', 'LRCX', 'INTC', 'GLD', 'TLT', 'VIXY']\n",
      "\n",
      "Benchmark weights (first few):\n",
      "NVDA    61.65%\n",
      "AMD      3.91%\n",
      "ASML      4.1%\n",
      "AMAT     1.82%\n",
      "TSM     14.18%\n",
      "LRCX     1.74%\n",
      "INTC     1.52%\n",
      "GLD      3.46%\n",
      "TLT      6.93%\n",
      "VIXY     0.69%\n",
      "Name: 0, dtype: object\n",
      "\n",
      "Î´ (Market): 3.727700 | Î¼_bench: 0.6198 | Ïƒ_bench: 0.4078\n",
      "\n",
      "Ï€ (sample) â€” head:\n",
      "NVDA    0.798106\n",
      "AMD     0.577738\n",
      "ASML    0.461463\n",
      "AMAT    0.472961\n",
      "TSM     0.462938\n",
      "Name: pi, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T19:04:53.505785Z",
     "start_time": "2025-08-19T19:04:53.372741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Step 6 â€” Cell 2: Ledoitâ€“Wolf shrinkage Î£, Ï€ under LW, PSD checks ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.linalg import eigvalsh\n",
    "\n",
    "# Uses R, assets, w_mcap, FREQ, deltas, Sigma_sample, mu_sample from Cell 1\n",
    "\n",
    "def ledoit_wolf_identity_shrinkage(returns: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Ledoitâ€“Wolf shrinkage toward identity-scaled target F = mu*I, mu=trace(S)/p.\n",
    "    Returns (Sigma_lw_annualized as DataFrame aligned to returns.columns, alpha).\n",
    "    \"\"\"\n",
    "    X = returns.values - returns.values.mean(axis=0, keepdims=True)\n",
    "    T, p = X.shape\n",
    "    if T <= 1:\n",
    "        raise ValueError(\"Not enough observations for LW shrinkage.\")\n",
    "    # daily sample covariance (unbiased by population convention for LW derivation)\n",
    "    S = (X.T @ X) / T\n",
    "\n",
    "    mu = np.trace(S) / p\n",
    "    F = mu * np.eye(p)\n",
    "\n",
    "    # phi-hat: average squared deviation of sample cov elements\n",
    "    # Using LW 2004 identity-target formula\n",
    "    X2 = X**2\n",
    "    term1 = (X2.T @ X2) / T\n",
    "    phi_mat = term1 - (S**2)\n",
    "    phi = phi_mat.sum()\n",
    "\n",
    "    # gamma-hat\n",
    "    gamma = np.sum((S - F)**2)\n",
    "\n",
    "    # rho-hat for identity target\n",
    "    rho = 0.0\n",
    "    for i in range(p):\n",
    "        rho += (S[i, i] - mu)**2\n",
    "    rho = rho\n",
    "\n",
    "    kappa = (phi - rho) / (gamma + 1e-16)\n",
    "    alpha = max(0.0, min(1.0, kappa / T))\n",
    "\n",
    "    Sigma_lw_daily = alpha * F + (1.0 - alpha) * S\n",
    "    Sigma_lw_annual = Sigma_lw_daily * FREQ\n",
    "    Sigma_lw_df = pd.DataFrame(Sigma_lw_annual, index=returns.columns, columns=returns.columns)\n",
    "    return Sigma_lw_df, float(alpha)\n",
    "\n",
    "# Compute LW Î£ on daily excess returns R\n",
    "Sigma_lw, alpha = ledoit_wolf_identity_shrinkage(R)\n",
    "\n",
    "# Equilibrium Ï€ under LW (Î´_Market from Cell 1)\n",
    "pi_lw = Sigma_lw.loc[assets, assets] @ w_mcap.loc[assets] * deltas[\"Market\"]\n",
    "pi_lw.name = \"pi\"\n",
    "\n",
    "# PSD diagnostics\n",
    "min_eig_sample = float(eigvalsh(Sigma_sample.loc[assets, assets].values).min())\n",
    "min_eig_lw     = float(eigvalsh(Sigma_lw.loc[assets, assets].values).min())\n",
    "\n",
    "# Write Overleaf artifact\n",
    "(OUTDIR / \"step6_pi_equilibrium_lw.tex\").write_text(pi_lw.to_frame(\"Value\").to_latex(escape=False, float_format=lambda x: f\"{x: .6f}\"))\n",
    "\n",
    "print(f\"Ledoitâ€“Wolf alpha (shrink to identity): {alpha:.3f}\")\n",
    "print(f\"min eigenvalue Î£_sample: {min_eig_sample:.6e}\")\n",
    "print(f\"min eigenvalue Î£_lw    : {min_eig_lw:.6e}\")\n",
    "print(\"\\nÏ€ (LW) â€” head:\")\n",
    "print(pi_lw.round(6).head())\n"
   ],
   "id": "1d13c9fe009baf7c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ledoitâ€“Wolf alpha (shrink to identity): 0.022\n",
      "min eigenvalue Î£_sample: 1.461386e-02\n",
      "min eigenvalue Î£_lw    : 1.873246e-02\n",
      "\n",
      "Ï€ (LW) â€” head:\n",
      "NVDA    0.789754\n",
      "AMD     0.564924\n",
      "ASML    0.451390\n",
      "AMAT    0.462242\n",
      "TSM     0.454507\n",
      "Name: pi, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T19:06:43.483304Z",
     "start_time": "2025-08-19T19:06:43.398043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Step 6 â€” Cell 3: Views (P, Q, Î©) and Ï„ table ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Uses: assets, R, Sigma_sample, OUTDIR, deltas from previous cells\n",
    "\n",
    "# ---- Ï„ values ----\n",
    "T_obs = len(R)\n",
    "tau_values = {\"tau_1_over_T\": 1.0 / max(T_obs, 1), \"tau_0_025\": 0.025}\n",
    "tau_df = pd.Series({\"tau_1_over_T\": f\"1/{T_obs}\", \"tau_0_025\": 0.025}, name=\"tau\")\n",
    "(OUTDIR / \"step6_tau_sensitivity.tex\").write_text(tau_df.to_frame().to_latex(escape=False))\n",
    "\n",
    "# ---- View specs (annualised, excess) ----\n",
    "VIEW_RELATIVE_EQUIP_VS_FABLESS = 0.03   # +3% p.a.\n",
    "VIEW_ABS_VIXY = -0.15                   # -15% p.a.\n",
    "VIEW_ABS_TLT  = 0.03                    # +3% p.a.\n",
    "\n",
    "CONFIDENCES = {\n",
    "    \"rel_equip_gt_fabless\": 0.70,   # v1\n",
    "    \"abs_vixy_negative\":    0.85,   # v2\n",
    "    \"abs_tlt_positive\":     0.60,   # v3\n",
    "}\n",
    "\n",
    "# ---- Build P and Q ----\n",
    "N = len(assets)\n",
    "idx = {a:i for i,a in enumerate(assets)}\n",
    "P = np.zeros((3, N))\n",
    "Q = np.zeros(3)\n",
    "\n",
    "# v1: (ASML+AMAT+LRCX)/3 - (NVDA+AMD)/2 = +3%\n",
    "equip = [\"ASML\",\"AMAT\",\"LRCX\"]\n",
    "fab   = [\"NVDA\",\"AMD\"]\n",
    "for e in equip:\n",
    "    if e in idx: P[0, idx[e]] =  1.0/len(equip)\n",
    "for f in fab:\n",
    "    if f in idx: P[0, idx[f]] = -1.0/len(fab)\n",
    "Q[0] = VIEW_RELATIVE_EQUIP_VS_FABLESS\n",
    "\n",
    "# v2: VIXY absolute = -15%\n",
    "if \"VIXY\" in idx: P[1, idx[\"VIXY\"]] = 1.0\n",
    "Q[1] = VIEW_ABS_VIXY\n",
    "\n",
    "# v3: TLT absolute = +3%\n",
    "if \"TLT\" in idx: P[2, idx[\"TLT\"]] = 1.0\n",
    "Q[2] = VIEW_ABS_TLT\n",
    "\n",
    "# ---- Map confidences to Î© for tau = 1/T (write this one to Overleaf) ----\n",
    "tau = tau_values[\"tau_1_over_T\"]\n",
    "PSigPt = P @ (tau * Sigma_sample.loc[assets, assets].values) @ P.T\n",
    "base_diag = np.clip(np.diag(PSigPt), 1e-12, None)\n",
    "\n",
    "c = np.array([\n",
    "    CONFIDENCES[\"rel_equip_gt_fabless\"],\n",
    "    CONFIDENCES[\"abs_vixy_negative\"],\n",
    "    CONFIDENCES[\"abs_tlt_positive\"],\n",
    "], dtype=float)\n",
    "nu = (1.0 - c) / np.clip(c, 1e-8, None)  # lower nu => higher confidence\n",
    "\n",
    "Omega = np.diag(nu * base_diag)\n",
    "\n",
    "# ---- Write Overleaf artifacts ----\n",
    "# P matrix with labels\n",
    "P_df = pd.DataFrame(P, index=[\"v1\",\"v2\",\"v3\"], columns=assets)\n",
    "(OUTDIR / \"step6_views_P.tex\").write_text(P_df.to_latex(escape=False, float_format=lambda x: f\"{x: .6f}\"))\n",
    "\n",
    "# Q, confidences, nu, and Omega diag\n",
    "qo_df = pd.DataFrame({\n",
    "    \"Q (annual)\": Q,\n",
    "    \"Confidence\": c,\n",
    "    \"nu=(1-c)/c\": nu,\n",
    "    \"Omega_diag\": np.diag(Omega),\n",
    "}, index=[\"v1\",\"v2\",\"v3\"])\n",
    "(OUTDIR / \"step6_views_QOmega.tex\").write_text(qo_df.to_latex(escape=False, float_format=lambda x: f\"{x: .6f}\"))\n",
    "\n",
    "print(\"Views built.\")\n",
    "print(\"\\nP (rows v1..v3, cols assets):\")\n",
    "print(P_df)\n",
    "\n",
    "print(\"\\nQ (annual):\", np.round(Q, 4))\n",
    "print(\"Confidences:\", np.round(c, 2), \"  nu:\", np.round(nu, 3))\n",
    "print(\"Omega diag (tau=1/T):\", np.round(np.diag(Omega), 6))\n"
   ],
   "id": "c1823769564e82f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Views built.\n",
      "\n",
      "P (rows v1..v3, cols assets):\n",
      "    NVDA  AMD      ASML      AMAT  TSM      LRCX  INTC  GLD  TLT  VIXY\n",
      "v1  -0.5 -0.5  0.333333  0.333333  0.0  0.333333   0.0  0.0  0.0   0.0\n",
      "v2   0.0  0.0  0.000000  0.000000  0.0  0.000000   0.0  0.0  0.0   1.0\n",
      "v3   0.0  0.0  0.000000  0.000000  0.0  0.000000   0.0  0.0  1.0   0.0\n",
      "\n",
      "Q (annual): [ 0.03 -0.15  0.03]\n",
      "Confidences: [0.7  0.85 0.6 ]   nu: [0.429 0.176 0.667]\n",
      "Omega diag (tau=1/T): [5.40e-05 1.17e-04 2.40e-05]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T19:08:11.451465Z",
     "start_time": "2025-08-19T19:08:11.011883Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Step 6 â€” Cell 4: BL posterior Î¼ (sample Î£, Ï„ in {1/T, 0.025}) ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Uses from previous cells: assets, R, mu_sample, Sigma_sample, pi_sample,\n",
    "# P, Q, CONFIDENCES, tau_values, OUTDIR\n",
    "\n",
    "def build_omega(P, Sigma, tau, confidences):\n",
    "    \"\"\"Diagonal Î© using nu=(1-c)/c scaled by diag(P (tau Î£) P^T).\"\"\"\n",
    "    PSigPt = P @ (tau * Sigma) @ P.T\n",
    "    base_diag = np.clip(np.diag(PSigPt), 1e-12, None)\n",
    "    c = np.array([confidences[\"rel_equip_gt_fabless\"],\n",
    "                  confidences[\"abs_vixy_negative\"],\n",
    "                  confidences[\"abs_tlt_positive\"]], dtype=float)\n",
    "    nu = (1.0 - c) / np.clip(c, 1e-8, None)\n",
    "    return np.diag(nu * base_diag)\n",
    "\n",
    "def bl_posterior(mu_prior, Sigma, P, Q, Omega, tau):\n",
    "    \"\"\"Standard BL closed form.\"\"\"\n",
    "    Sigma_tau_inv = np.linalg.inv(tau * Sigma)\n",
    "    mid = Sigma_tau_inv + P.T @ np.linalg.inv(Omega) @ P\n",
    "    rhs = Sigma_tau_inv @ mu_prior + P.T @ np.linalg.inv(Omega) @ Q\n",
    "    return np.linalg.solve(mid, rhs)\n",
    "\n",
    "# Compute posterior for each tau\n",
    "posteriors = {}\n",
    "for tau_name, tau_val in tau_values.items():\n",
    "    Omega_tau = build_omega(P, Sigma_sample.loc[assets, assets].values, tau_val, CONFIDENCES)\n",
    "    mu_post = bl_posterior(\n",
    "        mu_prior=pi_sample.loc[assets].values,\n",
    "        Sigma=Sigma_sample.loc[assets, assets].values,\n",
    "        P=P,\n",
    "        Q=Q,\n",
    "        Omega=Omega_tau,\n",
    "        tau=tau_val\n",
    "    )\n",
    "    s = pd.Series(mu_post, index=assets, name=f\"Posterior ({tau_name})\")\n",
    "    posteriors[tau_name] = s\n",
    "    # Write individual vectors for Overleaf\n",
    "    if tau_name == \"tau_1_over_T\":\n",
    "        (OUTDIR / \"step6_posterior_mu_sample_tau1overT.tex\").write_text(\n",
    "            s.to_frame(\"Value\").to_latex(escape=False, float_format=lambda x: f\"{x: .6f}\")\n",
    "        )\n",
    "    else:\n",
    "        (OUTDIR / \"step6_posterior_mu_sample_tau_0_025.tex\").write_text(\n",
    "            s.to_frame(\"Value\").to_latex(escape=False, float_format=lambda x: f\"{x: .6f}\")\n",
    "        )\n",
    "\n",
    "# Comparison table (Naive mean vs Ï€ vs Posterior columns)\n",
    "cmp = pd.DataFrame({\n",
    "    \"Naive (mean)\": mu_sample.loc[assets],\n",
    "    \"Equilibrium Ï€\": pi_sample.loc[assets],\n",
    "    \"Posterior (Ï„=1/T)\": posteriors[\"tau_1_over_T\"],\n",
    "    \"Posterior (Ï„=0.025)\": posteriors[\"tau_0_025\"],\n",
    "})\n",
    "(OUTDIR / \"step6_mu_compare.tex\").write_text(cmp.to_latex(escape=False, float_format=lambda x: f\"{x: .6f}\"))\n",
    "\n",
    "# Quick sanity: bar chart of Naive vs Ï€ vs Posterior (Ï„=1/T)\n",
    "plt.figure(figsize=(9,3.8))\n",
    "width = 0.25\n",
    "x = np.arange(len(assets))\n",
    "plt.bar(x - width, cmp[\"Naive (mean)\"].values, width, label=\"Naive\")\n",
    "plt.bar(x,         cmp[\"Equilibrium Ï€\"].values, width, label=\"Ï€\")\n",
    "plt.bar(x + width, cmp[\"Posterior (Ï„=1/T)\"].values, width, label=\"Posterior\")\n",
    "plt.xticks(x, assets, rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Annualised excess return\")\n",
    "plt.title(\"BL: Naive vs Equilibrium Ï€ vs Posterior (sample Î£)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTDIR / \"step6_mu_bar_sample.png\"); plt.close()\n",
    "\n",
    "# Console sanity\n",
    "print(\"Posterior Î¼ (Ï„=1/T) â€” head:\\n\", posteriors[\"tau_1_over_T\"].round(6).head())\n",
    "print(\"\\nCorrelations among vectors:\")\n",
    "print(pd.DataFrame({\n",
    "    \"Naive_vs_pi\": cmp[\"Naive (mean)\"].corr(cmp[\"Equilibrium Ï€\"]),\n",
    "    \"Naive_vs_Post(1/T)\": cmp[\"Naive (mean)\"].corr(cmp[\"Posterior (Ï„=1/T)\"]),\n",
    "    \"pi_vs_Post(1/T)\": cmp[\"Equilibrium Ï€\"].corr(cmp[\"Posterior (Ï„=1/T)\"]),\n",
    "}, index=[\"corr\"]).T.round(3))\n",
    "print(\"\\nWrote: step6_posterior_mu_sample_tau1overT.tex, step6_posterior_mu_sample_tau_0_025.tex, step6_mu_compare.tex, step6_mu_bar_sample.png\")\n"
   ],
   "id": "a397e9bdc412e9dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posterior Î¼ (Ï„=1/T) â€” head:\n",
      " NVDA    0.538897\n",
      "AMD     0.352109\n",
      "ASML    0.395745\n",
      "AMAT    0.405588\n",
      "TSM     0.343651\n",
      "Name: Posterior (tau_1_over_T), dtype: float64\n",
      "\n",
      "Correlations among vectors:\n",
      "                     corr\n",
      "Naive_vs_pi         0.894\n",
      "Naive_vs_Post(1/T)  0.860\n",
      "pi_vs_Post(1/T)     0.975\n",
      "\n",
      "Wrote: step6_posterior_mu_sample_tau1overT.tex, step6_posterior_mu_sample_tau_0_025.tex, step6_mu_compare.tex, step6_mu_bar_sample.png\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T19:09:27.477140Z",
     "start_time": "2025-08-19T19:09:27.422621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Step 6 â€” Cell 5: optimiser scaffolding + Min-Variance (sample Î£) ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Uses: assets, Sigma_sample, OUTDIR, w_mcap from previous cells\n",
    "\n",
    "# ---- Bounds (long-only + caps) ----\n",
    "CAP_SEMI = 0.25\n",
    "CAP_GLD_TLT = 0.30\n",
    "CAP_VIXY = 0.05\n",
    "\n",
    "def make_bounds(assets):\n",
    "    b = []\n",
    "    for a in assets:\n",
    "        if a == \"VIXY\":\n",
    "            b.append((0.0, CAP_VIXY))\n",
    "        elif a in (\"GLD\",\"TLT\"):\n",
    "            b.append((0.0, CAP_GLD_TLT))\n",
    "        else:  # semis\n",
    "            b.append((0.0, CAP_SEMI))\n",
    "    return tuple(b)\n",
    "\n",
    "bounds = make_bounds(assets)\n",
    "\n",
    "# ---- Min-Variance solver (budget = 1) ----\n",
    "def min_variance(Sigma, bounds):\n",
    "    p = Sigma.shape[0]\n",
    "    def obj(w): return float(w @ Sigma @ w)\n",
    "    cons = [{\"type\":\"eq\", \"fun\": lambda w: np.sum(w) - 1.0}]\n",
    "    x0 = np.full(p, 1.0/p)\n",
    "    res = minimize(obj, x0=x0, method=\"SLSQP\", bounds=bounds, constraints=cons,\n",
    "                   options={\"maxiter\": 2000, \"ftol\": 1e-12})\n",
    "    return res\n",
    "\n",
    "Sigma_mat = Sigma_sample.loc[assets, assets].values\n",
    "res = min_variance(Sigma_mat, bounds)\n",
    "\n",
    "if not res.success:\n",
    "    print(\"Min-Variance failed:\", res.message)\n",
    "    # simple fallback: clip equal weights to bounds and renormalize\n",
    "    w = np.full(len(assets), 1.0/len(assets))\n",
    "    w = np.minimum(w, np.array([ub for (_,ub) in bounds]))\n",
    "    w = w / w.sum()\n",
    "else:\n",
    "    w = res.x\n",
    "\n",
    "w_minvar_sample = pd.Series(w, index=assets, name=\"MinVar (sample Î£)\")\n",
    "\n",
    "# ---- Write Overleaf table ----\n",
    "(OUTDIR / \"step6_w_MinVar_sample.tex\").write_text(\n",
    "    (w_minvar_sample*100).to_frame(\"% Weight\").to_latex(escape=False, float_format=lambda x: f\"{x: .2f}\")\n",
    ")\n",
    "\n",
    "# ---- Quick sanity prints ----\n",
    "sumw = float(w_minvar_sample.sum())\n",
    "viol_lo = (w_minvar_sample < -1e-9).sum()\n",
    "viol_hi = sum((w_minvar_sample.values - np.array([ub for (_,ub) in bounds])) > 1e-9)\n",
    "port_vol = np.sqrt(float(w_minvar_sample.values @ Sigma_mat @ w_minvar_sample.values))\n",
    "\n",
    "print(\"Min-Var â€” summary\")\n",
    "print(f\"  Sum of weights: {sumw:.6f}\")\n",
    "print(f\"  Lower bound violations: {viol_lo} | Upper bound violations: {viol_hi}\")\n",
    "print(f\"  Annualised volatility: {port_vol:.4f}\")\n",
    "print(\"\\nWeights (%):\")\n",
    "print((w_minvar_sample*100).round(2))\n"
   ],
   "id": "175e8377e7e23be6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min-Var â€” summary\n",
      "  Sum of weights: 1.000000\n",
      "  Lower bound violations: 0 | Upper bound violations: 0\n",
      "  Annualised volatility: 0.1383\n",
      "\n",
      "Weights (%):\n",
      "NVDA     0.00\n",
      "AMD      0.00\n",
      "ASML     2.01\n",
      "AMAT     5.17\n",
      "TSM     18.79\n",
      "LRCX     0.00\n",
      "INTC     9.03\n",
      "GLD     30.00\n",
      "TLT     30.00\n",
      "VIXY     5.00\n",
      "Name: MinVar (sample Î£), dtype: float64\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T19:12:12.631480Z",
     "start_time": "2025-08-19T19:12:12.551903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Step 6 â€” Cell 6: Meanâ€“Variance (Trustee/Market/Kelly) with BL posterior Î¼ (Ï„=1/T, sample Î£) ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from pathlib import Path\n",
    "\n",
    "# Uses from previous cells: assets, Sigma_sample, deltas, OUTDIR,\n",
    "# posteriors (dict from Cell 4), w_mcap, bounds (from Cell 5)\n",
    "\n",
    "# ----- Config: optional exposure floor on the semi sleeve -----\n",
    "SEMI_FLOOR = 0.0   # set to 0.60 if you want \"sum of semis â‰¥ 60%\"\n",
    "\n",
    "semi_idx = [i for i,a in enumerate(assets) if a not in (\"GLD\",\"TLT\",\"VIXY\")]\n",
    "\n",
    "def mean_variance(mu, Sigma, delta, bounds, semi_floor=0.0):\n",
    "    mu = np.asarray(mu); Sigma = np.asarray(Sigma)\n",
    "    p = len(mu)\n",
    "    def obj(w):  # 0.5*Î´*w'Î£w âˆ’ Î¼'w\n",
    "        return 0.5 * delta * float(w @ Sigma @ w) - float(mu @ w)\n",
    "    cons = [{\"type\":\"eq\", \"fun\": lambda w: np.sum(w) - 1.0}]\n",
    "    if semi_floor > 0 and len(semi_idx) > 0:\n",
    "        cons.append({\"type\":\"ineq\", \"fun\": lambda w: np.sum(w[semi_idx]) - semi_floor})\n",
    "    x0 = np.full(p, 1.0/p)\n",
    "    res = minimize(obj, x0=x0, method=\"SLSQP\", bounds=bounds, constraints=cons,\n",
    "                   options={\"maxiter\": 4000, \"ftol\": 1e-12})\n",
    "    return res\n",
    "\n",
    "# Posterior Î¼ (Ï„=1/T)\n",
    "mu_post = posteriors[\"tau_1_over_T\"].loc[assets].values\n",
    "Sigma_mat = Sigma_sample.loc[assets, assets].values\n",
    "\n",
    "def write_weights_tex(name, w_vec):\n",
    "    s = pd.Series(w_vec, index=assets, name=name)\n",
    "    (OUTDIR / f\"step6_w_MV_sample_{name}.tex\").write_text(\n",
    "        (s*100).to_frame(\"% Weight\").to_latex(escape=False, float_format=lambda x: f\"{x: .2f}\")\n",
    "    )\n",
    "    return s\n",
    "\n",
    "summ_rows = []\n",
    "for name, delta in deltas.items():\n",
    "    res = mean_variance(mu_post, Sigma_mat, delta, bounds, semi_floor=SEMI_FLOOR)\n",
    "    if not res.success:\n",
    "        print(f\"MVO ({name}) failed: {res.message}\")\n",
    "        continue\n",
    "    w = res.x\n",
    "    s = write_weights_tex(name, w)\n",
    "\n",
    "    port_mu = float(mu_post @ w)\n",
    "    port_vol = float(np.sqrt(w @ Sigma_mat @ w))\n",
    "    sr = port_mu / (port_vol + 1e-16)\n",
    "    semi_sum = float(np.sum(w[semi_idx]))\n",
    "    bounds_hi = np.array([ub for (_,ub) in bounds])\n",
    "    ub_hits = int(np.sum((w > bounds_hi - 1e-6)))\n",
    "    lb_hits = int(np.sum((w < 1e-6)))\n",
    "    summ_rows.append([name, port_mu, port_vol, sr, semi_sum, ub_hits, lb_hits])\n",
    "\n",
    "summary = pd.DataFrame(summ_rows, columns=[\"Delta\", \"AnnExpRet\", \"AnnVol\", \"Sharpe\", \"SemiSleeve\", \"UpperBoundHits\", \"NearZeroWeights\"])\n",
    "print(\"\\nMVO summary (sample Î£, Ï„=1/T):\")\n",
    "print(summary.round(4))\n",
    "\n",
    "# Also print top-5 weights vs benchmark for the Market posture to sanity-check NVDA/AMD exposure\n",
    "if \"Market\" in deltas.index:\n",
    "    res_mkt = mean_variance(mu_post, Sigma_mat, deltas[\"Market\"], bounds, semi_floor=SEMI_FLOOR)\n",
    "    if res_mkt.success:\n",
    "        w_mkt = pd.Series(res_mkt.x, index=assets)\n",
    "        df = pd.DataFrame({\"MVO_Market\": w_mkt, \"MCAP_Bench\": w_mcap.loc[assets]})\n",
    "        df[\"Active\"] = df[\"MVO_Market\"] - df[\"MCAP_Bench\"]\n",
    "        print(\"\\nMVO (Market) â€” top 5 by weight:\")\n",
    "        print((df.sort_values(\"MVO_Market\", ascending=False).head(5) * 100).round(2))\n",
    "        print(\"\\nMVO (Market) â€” semi sleeve %:\", round(w_mkt.iloc[semi_idx].sum()*100, 2))\n"
   ],
   "id": "8ce4faf81396857b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MVO summary (sample Î£, Ï„=1/T):\n",
      "     Delta  AnnExpRet  AnnVol  Sharpe  SemiSleeve  UpperBoundHits  \\\n",
      "0  Trustee     0.1983  0.1797  1.1034      0.4393               1   \n",
      "1   Market     0.3316  0.2922  1.1349      0.7645               2   \n",
      "2    Kelly     0.4365  0.4016  1.0868      1.0000               3   \n",
      "\n",
      "   NearZeroWeights  \n",
      "0                2  \n",
      "1                2  \n",
      "2                5  \n",
      "\n",
      "MVO (Market) â€” top 5 by weight:\n",
      "      MVO_Market  MCAP_Bench  Active\n",
      "NVDA       25.00       61.65  -36.65\n",
      "ASML       15.87        4.10   11.77\n",
      "AMAT       14.13        1.82   12.31\n",
      "TLT        11.49        6.93    4.56\n",
      "TSM        11.16       14.18   -3.03\n",
      "\n",
      "MVO (Market) â€” semi sleeve %: 76.45\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T19:13:23.644684Z",
     "start_time": "2025-08-19T19:13:23.508750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Step 6 â€” Cell 7: Max Sharpe (sample Î£, Ï„=1/T) ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from pathlib import Path\n",
    "\n",
    "# Uses: assets, OUTDIR, bounds (from Cell 5)\n",
    "#       posteriors (from Cell 4), Sigma_sample, w_mcap\n",
    "\n",
    "mu_post = posteriors[\"tau_1_over_T\"].loc[assets].values\n",
    "Sigma_mat = Sigma_sample.loc[assets, assets].values\n",
    "\n",
    "def sharpe(mu, Sigma, w):\n",
    "    num = float(mu @ w)\n",
    "    den = np.sqrt(float(w @ Sigma @ w))\n",
    "    return num / (den + 1e-16)\n",
    "\n",
    "def max_sharpe(mu, Sigma, bounds):\n",
    "    p = len(mu)\n",
    "    def obj(w):  # maximize Sharpe -> minimize negative Sharpe\n",
    "        return -sharpe(mu, Sigma, w)\n",
    "    cons = [{\"type\":\"eq\", \"fun\": lambda w: np.sum(w) - 1.0}]\n",
    "    x0 = np.full(p, 1.0/p)\n",
    "    res = minimize(obj, x0=x0, method=\"SLSQP\", bounds=bounds, constraints=cons,\n",
    "                   options={\"maxiter\": 4000, \"ftol\": 1e-12})\n",
    "    return res\n",
    "\n",
    "res = max_sharpe(mu_post, Sigma_mat, bounds)\n",
    "if not res.success:\n",
    "    print(\"Max Sharpe failed:\", res.message)\n",
    "else:\n",
    "    w_ms = pd.Series(res.x, index=assets, name=\"MaxSharpe (sample Î£)\")\n",
    "    # Write Overleaf\n",
    "    (OUTDIR / \"step6_w_MS_sample_Market.tex\").write_text(\n",
    "        (w_ms*100).to_frame(\"% Weight\").to_latex(escape=False, float_format=lambda x: f\"{x: .2f}\")\n",
    "    )\n",
    "\n",
    "    # Active bar vs benchmark\n",
    "    active = (w_ms - w_mcap.loc[assets])\n",
    "    plt.figure(figsize=(9,3.4))\n",
    "    plt.bar(active.index, active.values)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.ylabel(\"Active Weight\")\n",
    "    plt.title(\"Active Weights: Max Sharpe (sample Î£, Ï„=1/T)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTDIR / \"step6_active_MS_Market.png\"); plt.close()\n",
    "\n",
    "    # Summary stats\n",
    "    port_mu = float(mu_post @ w_ms.values)\n",
    "    port_vol = float(np.sqrt(w_ms.values @ Sigma_mat @ w_ms.values))\n",
    "    sr = port_mu / (port_vol + 1e-16)\n",
    "    semi_idx = [i for i,a in enumerate(assets) if a not in (\"GLD\",\"TLT\",\"VIXY\")]\n",
    "    semi_sum = float(w_ms.values[semi_idx].sum())\n",
    "\n",
    "    bounds_hi = np.array([ub for (_,ub) in bounds])\n",
    "    ub_hits = int(np.sum((w_ms.values > bounds_hi - 1e-6)))\n",
    "    lb_hits = int(np.sum((w_ms.values < 1e-6)))\n",
    "\n",
    "    print(\"Max Sharpe â€” summary\")\n",
    "    print(f\"  AnnExpRet: {port_mu:.4f} | AnnVol: {port_vol:.4f} | Sharpe: {sr:.3f} | Semi sleeve: {semi_sum*100:.2f}%\")\n",
    "    print(f\"  UpperBoundHits: {ub_hits} | NearZeroWeights: {lb_hits}\")\n",
    "    print(\"\\nWeights (%):\")\n",
    "    print((w_ms*100).round(2))\n",
    "\n",
    "    # Top-5 vs benchmark\n",
    "    comp = pd.DataFrame({\"MS\": w_ms, \"MCAP\": w_mcap.loc[assets]})\n",
    "    comp[\"Active\"] = comp[\"MS\"] - comp[\"MCAP\"]\n",
    "    print(\"\\nTop 5 by MS weight (%):\")\n",
    "    print((comp.sort_values(\"MS\", ascending=False).head(5)*100).round(2))\n"
   ],
   "id": "87afd931ae9ea46c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Sharpe â€” summary\n",
      "  AnnExpRet: 0.2962 | AnnVol: 0.2603 | Sharpe: 1.138 | Semi sleeve: 66.60%\n",
      "  UpperBoundHits: 2 | NearZeroWeights: 2\n",
      "\n",
      "Weights (%):\n",
      "NVDA    25.00\n",
      "AMD      0.00\n",
      "ASML    12.87\n",
      "AMAT    11.70\n",
      "TSM      8.62\n",
      "LRCX     8.40\n",
      "INTC     0.00\n",
      "GLD     12.98\n",
      "TLT     15.43\n",
      "VIXY     5.00\n",
      "Name: MaxSharpe (sample Î£), dtype: float64\n",
      "\n",
      "Top 5 by MS weight (%):\n",
      "         MS   MCAP  Active\n",
      "NVDA  25.00  61.65  -36.65\n",
      "TLT   15.43   6.93    8.50\n",
      "GLD   12.98   3.46    9.51\n",
      "ASML  12.87   4.10    8.77\n",
      "AMAT  11.70   1.82    9.88\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T19:20:18.314512Z",
     "start_time": "2025-08-19T19:20:03.930798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Step 6 â€” Cell 8 (fixed): Frontiers (Ï€ vs BL Î¼) + Active Risk (MVO Market) ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from pathlib import Path\n",
    "\n",
    "# Uses from earlier cells: assets, OUTDIR, Sigma_sample, pi_sample, posteriors, w_mcap, bounds,\n",
    "# and (ideally) mu_sample. No reading .tex files here.\n",
    "\n",
    "Sigma_mat = Sigma_sample.loc[assets, assets].values\n",
    "mu_pi     = pi_sample.loc[assets].values                      # prior (equilibrium)\n",
    "mu_post   = posteriors[\"tau_1_over_T\"].loc[assets].values     # posterior (with views)\n",
    "\n",
    "def min_var_at_target(mu_target, mu_vec, Sigma, bounds):\n",
    "    p = len(mu_vec)\n",
    "    def obj(w): return float(w @ Sigma @ w)\n",
    "    cons = [\n",
    "        {\"type\":\"eq\", \"fun\": lambda w: np.sum(w) - 1.0},\n",
    "        {\"type\":\"eq\", \"fun\": lambda w: float(mu_vec @ w) - float(mu_target)},\n",
    "    ]\n",
    "    x0 = np.full(p, 1.0/p)\n",
    "    return minimize(obj, x0=x0, method=\"SLSQP\", bounds=bounds, constraints=cons,\n",
    "                    options={\"maxiter\": 4000, \"ftol\": 1e-12})\n",
    "\n",
    "def make_frontier(mu_vec, Sigma, bounds, n=45):\n",
    "    mu_lo, mu_hi = float(mu_vec.min()), float(mu_vec.max())\n",
    "    targets = np.linspace(mu_lo*0.5, mu_hi*1.05, n)\n",
    "    xs, ys = [], []\n",
    "    for t in targets:\n",
    "        res = min_var_at_target(t, mu_vec, Sigma, bounds)\n",
    "        if res.success:\n",
    "            w = res.x\n",
    "            xs.append(np.sqrt(float(w @ Sigma @ w)))\n",
    "            ys.append(float(mu_vec @ w))\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "# Build both frontiers\n",
    "x_pi,   y_pi   = make_frontier(mu_pi,   Sigma_mat, bounds)\n",
    "x_post, y_post = make_frontier(mu_post, Sigma_mat, bounds)\n",
    "\n",
    "plt.figure(figsize=(6.0,4.4))\n",
    "plt.plot(x_pi,   y_pi,   label=\"Frontier (Ï€ prior)\")\n",
    "plt.plot(x_post, y_post, label=\"Frontier (BL posterior)\")\n",
    "plt.xlabel(\"Volatility\"); plt.ylabel(\"Expected excess return\")\n",
    "plt.title(\"Efficient Frontiers (sample Î£)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTDIR / \"step6_frontier_sample.png\"); plt.close()\n",
    "\n",
    "# Recompute Î´_Market safely (prefer in-memory 'deltas', else recompute from Î¼, Î£, w_mcap)\n",
    "def get_delta_market():\n",
    "    try:\n",
    "        return float(deltas[\"Market\"])  # from Cell 1\n",
    "    except Exception:\n",
    "        mu_bench = float(mu_sample.loc[assets] @ w_mcap.loc[assets].values)  # annual excess\n",
    "        sigma2_bench = float(w_mcap.loc[assets].values @ Sigma_mat @ w_mcap.loc[assets].values)\n",
    "        return (mu_bench / sigma2_bench) if sigma2_bench > 0 else 10.0\n",
    "\n",
    "delta_market = get_delta_market()\n",
    "\n",
    "# MVO (Market) with posterior Î¼\n",
    "def mvo(mu_vec, Sigma, delta, bounds):\n",
    "    p = len(mu_vec)\n",
    "    def obj(w): return 0.5*delta*float(w @ Sigma @ w) - float(mu_vec @ w)\n",
    "    cons = [{\"type\":\"eq\", \"fun\": lambda w: np.sum(w) - 1.0}]\n",
    "    x0 = np.full(p, 1.0/p)\n",
    "    return minimize(obj, x0=x0, method=\"SLSQP\", bounds=bounds, constraints=cons,\n",
    "                    options={\"maxiter\": 4000, \"ftol\": 1e-12})\n",
    "\n",
    "res = mvo(mu_post, Sigma_mat, delta_market, bounds)\n",
    "if not res.success:\n",
    "    raise RuntimeError(f\"MVO (Market) failed: {res.message}\")\n",
    "\n",
    "w_mvo = pd.Series(res.x, index=assets, name=\"MVO_Market\")\n",
    "\n",
    "# Active stats vs benchmark\n",
    "d = w_mvo.values - w_mcap.loc[assets].values\n",
    "te = np.sqrt(float(d @ Sigma_mat @ d))\n",
    "act_mu = float(mu_post @ d)\n",
    "act_sr = act_mu / (te + 1e-16)\n",
    "\n",
    "active_tbl = pd.DataFrame(\n",
    "    {\"Active TE\": [te], \"Active ExpRet\": [act_mu], \"Active Sharpe\": [act_sr]},\n",
    "    index=[\"sample\"]\n",
    ")\n",
    "(OUTDIR / \"step6_active_risk_MV_Market.tex\").write_text(\n",
    "    active_tbl.to_latex(escape=False, float_format=lambda x: f\"{x: .4f}\")\n",
    ")\n",
    "\n",
    "# Active bar plot\n",
    "plt.figure(figsize=(9,3.4))\n",
    "active = (w_mvo - w_mcap.loc[assets])\n",
    "plt.bar(active.index, active.values)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Active Weight\")\n",
    "plt.title(\"Active Weights: MVO (Market, sample Î£)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTDIR / \"step6_active_MV_Market.png\"); plt.close()\n",
    "\n",
    "print(\"Wrote: step6_frontier_sample.png, step6_active_risk_MV_Market.tex, step6_active_MV_Market.png\")\n",
    "print(f\"Active TE={te:.4f}, Active ExpRet={act_mu:.4f}, Active Sharpe={act_sr:.3f}\")\n"
   ],
   "id": "27cce5f2f64a0961",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: step6_frontier_sample.png, step6_active_risk_MV_Market.tex, step6_active_MV_Market.png\n",
      "Active TE=0.1639, Active ExpRet=-0.0988, Active Sharpe=-0.603\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T19:22:19.336162Z",
     "start_time": "2025-08-19T19:22:18.435790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Step 6 â€” Cell 9: No-rebalancing backtest + rolling betas ===\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Reuse globals where possible; also reload what we need so this cell is standalone.\n",
    "OUTDIR = Path(\"outputs\"); OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "RET_FILE = OUTDIR / \"step2_excess_returns.csv\"\n",
    "FACTORS_FILE = OUTDIR / \"step4_all_factors.csv\"\n",
    "\n",
    "# ---- Load returns (daily excess) and align universe ----\n",
    "ret_all = pd.read_csv(RET_FILE, index_col=0, parse_dates=True)\n",
    "assets = [c for c in ret_all.columns if c in [\"NVDA\",\"AMD\",\"ASML\",\"AMAT\",\"TSM\",\"LRCX\",\"INTC\",\"GLD\",\"TLT\",\"VIXY\"]]\n",
    "R = ret_all[assets].copy()\n",
    "\n",
    "FREQ = 252\n",
    "\n",
    "# ---- Pull Î¼_post (Ï„=1/T) and Î£_sample from earlier cells; recompute if missing ----\n",
    "try:\n",
    "    mu_post_vec = posteriors[\"tau_1_over_T\"].loc[assets].values\n",
    "    Sigma_mat = Sigma_sample.loc[assets, assets].values\n",
    "except NameError:\n",
    "    # Quick recompute (rare, only if notebook state was reset)\n",
    "    mu_sample = R.mean()*FREQ\n",
    "    Sigma_sample = R.cov()*FREQ\n",
    "    # Fall back: use equilibrium Ï€ as prior and a neutral posterior (no views)\n",
    "    mu_post_vec = mu_sample.loc[assets].values\n",
    "    Sigma_mat = Sigma_sample.loc[assets, assets].values\n",
    "\n",
    "# ---- Bounds (must match earlier) ----\n",
    "CAP_SEMI, CAP_GLD_TLT, CAP_VIXY = 0.25, 0.30, 0.05\n",
    "def make_bounds(assets):\n",
    "    b = []\n",
    "    for a in assets:\n",
    "        if a == \"VIXY\":\n",
    "            b.append((0.0, CAP_VIXY))\n",
    "        elif a in (\"GLD\",\"TLT\"):\n",
    "            b.append((0.0, CAP_GLD_TLT))\n",
    "        else:\n",
    "            b.append((0.0, CAP_SEMI))\n",
    "    return tuple(b)\n",
    "bounds = make_bounds(assets)\n",
    "\n",
    "# ---- Benchmark weights (reuse if in memory; else rebuild fallback) ----\n",
    "try:\n",
    "    w_bench = w_mcap.loc[assets]\n",
    "except NameError:\n",
    "    # fallback logic from Cell 1\n",
    "    FALLBACK_MCAP = {\"NVDA\":3000.0,\"TSM\":1000.0,\"ASML\":300.0,\"AMD\":280.0,\"AMAT\":130.0,\"LRCX\":120.0,\"INTC\":150.0}\n",
    "    DIVERSIFIER_SLEEVE = {\"GLD\":0.06,\"TLT\":0.06,\"VIXY\":0.03}\n",
    "    w = pd.Series(0.0, index=assets, dtype=float)\n",
    "    semi_names = [a for a in assets if a in FALLBACK_MCAP]\n",
    "    sleeve = sum(DIVERSIFIER_SLEEVE.get(a,0.0) for a in assets); sleeve = min(sleeve, 0.20)\n",
    "    if semi_names:\n",
    "        semi_total = 1.0 - sleeve\n",
    "        semi_sum = sum(FALLBACK_MCAP[s] for s in semi_names)\n",
    "        for s in semi_names:\n",
    "            w.loc[s] = semi_total * (FALLBACK_MCAP[s]/semi_sum)\n",
    "    for a in assets:\n",
    "        if a in DIVERSIFIER_SLEEVE: w.loc[a] = DIVERSIFIER_SLEEVE[a]\n",
    "    w_bench = w / w.sum()\n",
    "\n",
    "# ---- Solvers (MVO with Î´_Market; Max Sharpe) ----\n",
    "def get_delta_market(mu_post_vec, Sigma_mat, w_bench):\n",
    "    mu_b = float(mu_post_vec @ w_bench.values)  # use posterior Î¼ for consistency\n",
    "    sig2_b = float(w_bench.values @ Sigma_mat @ w_bench.values)\n",
    "    return (mu_b / sig2_b) if sig2_b > 0 else 10.0\n",
    "\n",
    "def mean_variance(mu, Sigma, delta, bounds):\n",
    "    p = len(mu)\n",
    "    def obj(w): return 0.5*delta*float(w @ Sigma @ w) - float(mu @ w)\n",
    "    cons = [{\"type\":\"eq\", \"fun\": lambda w: np.sum(w) - 1.0}]\n",
    "    x0 = np.full(p, 1.0/p)\n",
    "    return minimize(obj, x0=x0, method=\"SLSQP\", bounds=bounds, constraints=cons,\n",
    "                    options={\"maxiter\": 4000, \"ftol\": 1e-12})\n",
    "\n",
    "def max_sharpe(mu, Sigma, bounds):\n",
    "    p = len(mu)\n",
    "    def shp(w):\n",
    "        num = float(mu @ w); den = np.sqrt(float(w @ Sigma @ w))\n",
    "        return num/(den+1e-16)\n",
    "    def obj(w): return -shp(w)\n",
    "    cons = [{\"type\":\"eq\", \"fun\": lambda w: np.sum(w) - 1.0}]\n",
    "    x0 = np.full(p, 1.0/p)\n",
    "    return minimize(obj, x0=x0, method=\"SLSQP\", bounds=bounds, constraints=cons,\n",
    "                    options={\"maxiter\": 4000, \"ftol\": 1e-12})\n",
    "\n",
    "delta_mkt = get_delta_market(mu_post_vec, Sigma_mat, w_bench)\n",
    "\n",
    "res_mvo = mean_variance(mu_post_vec, Sigma_mat, delta_mkt, bounds)\n",
    "res_ms  = max_sharpe(mu_post_vec, Sigma_mat, bounds)\n",
    "if (not res_mvo.success) or (not res_ms.success):\n",
    "    raise RuntimeError(f\"Optimiser failed. MVO success={res_mvo.success}, MS success={res_ms.success}\")\n",
    "\n",
    "w_mvo = pd.Series(res_mvo.x, index=assets, name=\"BL_MVO\")\n",
    "w_ms  = pd.Series(res_ms.x,  index=assets, name=\"BL_MS\")\n",
    "\n",
    "# ---- Build portfolio daily excess returns (fixed weights; no rebalancing) ----\n",
    "port_mvo = (R @ w_mvo).rename(\"BL_MVO\")\n",
    "port_ms  = (R @ w_ms).rename(\"BL_MS\")\n",
    "w_ew = pd.Series(1.0/len(assets), index=assets)\n",
    "ew_ret = (R @ w_ew).rename(\"EW\")\n",
    "mcap_ret = (R @ w_bench).rename(\"MCAP\")\n",
    "smh_ret = ret_all[\"SMH\"] if \"SMH\" in ret_all.columns else None\n",
    "\n",
    "# ---- Performance stats & table ----\n",
    "def perf_stats(ret: pd.Series):\n",
    "    r = ret.dropna()\n",
    "    if r.empty:\n",
    "        return dict(AnnRet=np.nan, AnnVol=np.nan, Sharpe=np.nan, MaxDD=np.nan)\n",
    "    mu = r.mean()*FREQ\n",
    "    vol = r.std()*np.sqrt(FREQ)\n",
    "    sr = mu/vol if vol>0 else np.nan\n",
    "    curve = (1+r).cumprod()\n",
    "    dd = (curve/curve.cummax() - 1.0).min()\n",
    "    return dict(AnnRet=float(mu), AnnVol=float(vol), Sharpe=float(sr), MaxDD=float(dd))\n",
    "\n",
    "perf = {\n",
    "    \"BL_MVO\": perf_stats(port_mvo),\n",
    "    \"BL_MS\":  perf_stats(port_ms),\n",
    "    \"EW\":     perf_stats(ew_ret),\n",
    "    \"MCAP\":   perf_stats(mcap_ret),\n",
    "}\n",
    "if smh_ret is not None:\n",
    "    perf[\"SMH\"] = perf_stats(smh_ret)\n",
    "\n",
    "df_perf = pd.DataFrame(perf).T\n",
    "df_perf_fmt = df_perf.copy()\n",
    "df_perf_fmt[\"AnnRet\"] *= 100; df_perf_fmt[\"AnnVol\"] *= 100; df_perf_fmt[\"MaxDD\"] *= 100\n",
    "df_perf_fmt.columns = [\"AnnRet (%)\",\"AnnVol (%)\",\"Sharpe\",\"MaxDD (%)\"]\n",
    "(OUTDIR / \"step6_perf_bl_vs_bench.tex\").write_text(\n",
    "    df_perf_fmt.to_latex(escape=False, float_format=lambda x: f\"{x: .2f}\")\n",
    ")\n",
    "\n",
    "# ---- Cumulative $10k chart ----\n",
    "def growth_of_10k(series): return 10000*(1+series.fillna(0)).cumprod()\n",
    "\n",
    "plt.figure(figsize=(8.4,4.2))\n",
    "plt.plot(growth_of_10k(port_mvo), label=\"BL_MVO\")\n",
    "plt.plot(growth_of_10k(port_ms),  label=\"BL_MS\")\n",
    "plt.plot(growth_of_10k(ew_ret),   label=\"EW\")\n",
    "plt.plot(growth_of_10k(mcap_ret), label=\"MCAP\")\n",
    "if smh_ret is not None: plt.plot(growth_of_10k(smh_ret), label=\"SMH\")\n",
    "plt.legend(); plt.title(\"Cumulative P&L (No Rebalancing)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTDIR / \"step6_cum_bl_vs_bench.png\"); plt.close()\n",
    "\n",
    "# ---- Rolling betas (optional) ----\n",
    "def rolling_beta(y: pd.Series, x: pd.Series, window=252):\n",
    "    df = pd.concat([y, x], axis=1).dropna()\n",
    "    if len(df) < window+5: return pd.Series(dtype=float)\n",
    "    betas, idxs = [], []\n",
    "    for i in range(window, len(df)):\n",
    "        sl = df.iloc[i-window:i]\n",
    "        X, Y = sl.iloc[:,1].values, sl.iloc[:,0].values\n",
    "        vx = np.var(X)\n",
    "        b = np.cov(X, Y, ddof=0)[0,1] / (vx + 1e-16)\n",
    "        betas.append(b); idxs.append(df.index[i])\n",
    "    return pd.Series(betas, index=idxs)\n",
    "\n",
    "# vs SMH, if available\n",
    "if smh_ret is not None:\n",
    "    beta_smh = rolling_beta(port_mvo, smh_ret)\n",
    "    if not beta_smh.empty:\n",
    "        plt.figure(figsize=(7.6,3.4))\n",
    "        plt.plot(beta_smh)\n",
    "        plt.title(\"Rolling 252-day Beta: BL MVO vs SMH\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(OUTDIR / \"step6_rolling_beta_BL_SMH.png\"); plt.close()\n",
    "\n",
    "# vs SemiCapex (if factor file present)\n",
    "try:\n",
    "    factors = pd.read_csv(FACTORS_FILE, index_col=0, parse_dates=True)\n",
    "    if \"SemiCapex\" in factors.columns:\n",
    "        semi_beta = rolling_beta(port_mvo, factors[\"SemiCapex\"])\n",
    "        if not semi_beta.empty:\n",
    "            plt.figure(figsize=(7.6,3.4))\n",
    "            plt.plot(semi_beta)\n",
    "            plt.title(\"Rolling 252-day Beta: BL MVO vs SemiCapex\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(OUTDIR / \"step6_rolling_beta_BL_SemiCapex.png\"); plt.close()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(\"Wrote: step6_perf_bl_vs_bench.tex, step6_cum_bl_vs_bench.png\",\n",
    "      \"(+ rolling beta PNGs if sources available)\")\n"
   ],
   "id": "d409c2f02166098a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: step6_perf_bl_vs_bench.tex, step6_cum_bl_vs_bench.png (+ rolling beta PNGs if sources available)\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T19:25:32.909326Z",
     "start_time": "2025-08-19T19:25:29.529258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Step 6 â€” Cell 10: LW posterior Î¼, LW frontier, MVO(Market) under LW, view stability ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from pathlib import Path\n",
    "\n",
    "OUTDIR = Path(\"outputs\"); OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Reuse variables if present; otherwise reconstruct minimal pieces\n",
    "try:\n",
    "    assets\n",
    "    Sigma_sample\n",
    "    Sigma_lw\n",
    "    posteriors\n",
    "    P, Q\n",
    "    tau_values\n",
    "    w_mcap\n",
    "    bounds\n",
    "except NameError:\n",
    "    # Minimal reloads (assumes prior cells were run in this session)\n",
    "    raise RuntimeError(\"Some earlier variables are missing; please run Cells 1â€“3 and 5â€“7 first.\")\n",
    "\n",
    "# --- build Omega for LW using tau=1/T ---\n",
    "T_obs = len(pd.read_csv(OUTDIR / \"step2_excess_returns.csv\", index_col=0, parse_dates=True))\n",
    "tau = 1.0 / max(T_obs, 1)\n",
    "\n",
    "def build_omega(P, Sigma, tau, confidences):\n",
    "    c = np.array([confidences[\"rel_equip_gt_fabless\"],\n",
    "                  confidences[\"abs_vixy_negative\"],\n",
    "                  confidences[\"abs_tlt_positive\"]], dtype=float)\n",
    "    nu = (1.0 - c) / np.clip(c, 1e-8, None)\n",
    "    PSigPt = P @ (tau * Sigma) @ P.T\n",
    "    base_diag = np.clip(np.diag(PSigPt), 1e-12, None)\n",
    "    return np.diag(nu * base_diag)\n",
    "\n",
    "CONFIDENCES = {\n",
    "    \"rel_equip_gt_fabless\": 0.70,\n",
    "    \"abs_vixy_negative\":    0.85,\n",
    "    \"abs_tlt_positive\":     0.60,\n",
    "}\n",
    "\n",
    "def bl_posterior(mu_prior, Sigma, P, Q, Omega, tau):\n",
    "    Sigma_tau_inv = np.linalg.inv(tau * Sigma)\n",
    "    mid = Sigma_tau_inv + P.T @ np.linalg.inv(Omega) @ P\n",
    "    rhs = Sigma_tau_inv @ mu_prior + P.T @ np.linalg.inv(Omega) @ Q\n",
    "    return np.linalg.solve(mid, rhs)\n",
    "\n",
    "# Priors (Ï€) already computed for both Î£; reuse\n",
    "pi_lw_vec = (Sigma_lw.loc[assets, assets] @ w_mcap.loc[assets] * float(pd.read_csv(OUTDIR / \"step6_delta_levels.tex\", sep=\"&\", header=None, engine=\"python\", on_bad_lines=\"skip\").iloc[1,2])\n",
    "             if False else (Sigma_lw.loc[assets, assets] @ w_mcap.loc[assets] * posteriors[\"tau_1_over_T\"].name if False else None))\n",
    "# Safer: recompute delta_market from memory (created in Cell 1/6/8)\n",
    "try:\n",
    "    delta_market = float(deltas[\"Market\"])\n",
    "except Exception:\n",
    "    # rebuild from sample Î¼/Î£ and w_mcap\n",
    "    mu_sample = pd.read_csv(OUTDIR / \"step2_excess_returns.csv\", index_col=0, parse_dates=True)[assets].mean()*252\n",
    "    Sigma_sample_tmp = pd.read_csv(OUTDIR / \"step2_excess_returns.csv\", index_col=0, parse_dates=True)[assets].cov()*252\n",
    "    mu_b = float(mu_sample.values @ w_mcap.loc[assets].values)\n",
    "    sig2_b = float(w_mcap.loc[assets].values @ Sigma_sample_tmp.values @ w_mcap.loc[assets].values)\n",
    "    delta_market = (mu_b/sig2_b) if sig2_b>0 else 10.0\n",
    "\n",
    "pi_lw = Sigma_lw.loc[assets, assets] @ w_mcap.loc[assets] * delta_market\n",
    "pi_lw = pd.Series(pi_lw, index=assets, name=\"pi_lw\")\n",
    "\n",
    "# Posterior Î¼ under LW, tau=1/T\n",
    "Omega_lw = build_omega(P, Sigma_lw.loc[assets, assets].values, tau, CONFIDENCES)\n",
    "mu_post_lw = bl_posterior(\n",
    "    mu_prior=pi_lw.values,\n",
    "    Sigma=Sigma_lw.loc[assets, assets].values,\n",
    "    P=P, Q=Q, Omega=Omega_lw, tau=tau\n",
    ")\n",
    "mu_post_lw = pd.Series(mu_post_lw, index=assets, name=\"Posterior_LW_tau_1_over_T\")\n",
    "(OUTDIR / \"step6_posterior_mu_lw_tau1overT.tex\").write_text(\n",
    "    mu_post_lw.to_frame(\"Value\").to_latex(escape=False, float_format=lambda x: f\"{x: .6f}\")\n",
    ")\n",
    "\n",
    "# View-stability table: compare sample vs LW posterior\n",
    "mu_post_sample = posteriors[\"tau_1_over_T\"].loc[assets]\n",
    "stab = pd.DataFrame({\n",
    "    \"Pearson\": [mu_post_sample.corr(mu_post_lw)],\n",
    "    \"Spearman\": [mu_post_sample.rank().corr(mu_post_lw.rank(), method=\"spearman\")]\n",
    "}, index=[\"sample_vs_lw\"])\n",
    "(OUTDIR / \"step6_view_stability.tex\").write_text(stab.to_latex(escape=False, float_format=lambda x: f\"{x: .3f}\"))\n",
    "\n",
    "# Frontier (LW)\n",
    "def min_var_at_target(mu_target, mu_vec, Sigma, bounds):\n",
    "    p = len(mu_vec)\n",
    "    def obj(w): return float(w @ Sigma @ w)\n",
    "    cons = [\n",
    "        {\"type\":\"eq\", \"fun\": lambda w: np.sum(w) - 1.0},\n",
    "        {\"type\":\"eq\", \"fun\": lambda w: float(mu_vec @ w) - float(mu_target)},\n",
    "    ]\n",
    "    x0 = np.full(p, 1.0/p)\n",
    "    return minimize(obj, x0=x0, method=\"SLSQP\", bounds=bounds, constraints=cons,\n",
    "                    options={\"maxiter\": 4000, \"ftol\": 1e-12})\n",
    "\n",
    "def make_frontier(mu_vec, Sigma, bounds, n=45):\n",
    "    mu_lo, mu_hi = float(mu_vec.min()), float(mu_vec.max())\n",
    "    targets = np.linspace(mu_lo*0.5, mu_hi*1.05, n)\n",
    "    xs, ys = [], []\n",
    "    for t in targets:\n",
    "        res = min_var_at_target(t, mu_vec, Sigma, bounds)\n",
    "        if res.success:\n",
    "            w = res.x\n",
    "            xs.append(np.sqrt(float(w @ Sigma @ w)))\n",
    "            ys.append(float(mu_vec @ w))\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "x_lw, y_lw = make_frontier(mu_post_lw.values, Sigma_lw.loc[assets, assets].values, bounds)\n",
    "\n",
    "plt.figure(figsize=(6.0,4.4))\n",
    "plt.plot(x_lw, y_lw, label=\"Frontier (BL posterior, LW Î£)\")\n",
    "plt.xlabel(\"Volatility\"); plt.ylabel(\"Expected excess return\")\n",
    "plt.title(\"Efficient Frontier (LW Î£)\")\n",
    "plt.legend(); plt.tight_layout()\n",
    "plt.savefig(OUTDIR / \"step6_frontier_lw.png\"); plt.close()\n",
    "\n",
    "# MVO (Market) under LW for completeness\n",
    "def mvo(mu_vec, Sigma, delta, bounds):\n",
    "    p = len(mu_vec)\n",
    "    def obj(w): return 0.5*delta*float(w @ Sigma @ w) - float(mu_vec @ w)\n",
    "    cons = [{\"type\":\"eq\", \"fun\": lambda w: np.sum(w) - 1.0}]\n",
    "    x0 = np.full(p, 1.0/p)\n",
    "    return minimize(obj, x0=x0, method=\"SLSQP\", bounds=bounds, constraints=cons,\n",
    "                    options={\"maxiter\": 4000, \"ftol\": 1e-12})\n",
    "\n",
    "res = mvo(mu_post_lw.values, Sigma_lw.loc[assets, assets].values, delta_market, bounds)\n",
    "if res.success:\n",
    "    w_lw_mvo = pd.Series(res.x, index=assets, name=\"MVO_LW_Market\")\n",
    "    (OUTDIR / \"step6_w_MV_lw_Market.tex\").write_text(\n",
    "        (w_lw_mvo*100).to_frame(\"% Weight\").to_latex(escape=False, float_format=lambda x: f\"{x: .2f}\")\n",
    "    )\n",
    "\n",
    "print(\"Wrote: step6_posterior_mu_lw_tau1overT.tex, step6_frontier_lw.png, step6_w_MV_lw_Market.tex, step6_view_stability.tex\")\n"
   ],
   "id": "6a3d3bb99f5e8d18",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: step6_posterior_mu_lw_tau1overT.tex, step6_frontier_lw.png, step6_w_MV_lw_Market.tex, step6_view_stability.tex\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T19:27:58.218061Z",
     "start_time": "2025-08-19T19:27:58.077973Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Step 6 â€” Cell 11: Risk contributions for BL MVO (Market) and Max Sharpe (sample Î£) ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy.optimize import minimize\n",
    "import warnings\n",
    "\n",
    "OUTDIR = Path(\"outputs\"); OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "RET_FILE = OUTDIR / \"step2_excess_returns.csv\"\n",
    "\n",
    "# Load returns to rebuild Î£ and align assets (keeps this cell standalone)\n",
    "ret_all = pd.read_csv(RET_FILE, index_col=0, parse_dates=True)\n",
    "assets = [c for c in ret_all.columns if c in [\"NVDA\",\"AMD\",\"ASML\",\"AMAT\",\"TSM\",\"LRCX\",\"INTC\",\"GLD\",\"TLT\",\"VIXY\"]]\n",
    "R = ret_all[assets].copy()\n",
    "FREQ = 252\n",
    "Sigma_sample = R.cov() * FREQ\n",
    "\n",
    "# Reuse posterior Î¼ (Ï„=1/T) if available; otherwise recompute neutral fallback\n",
    "try:\n",
    "    mu_post = posteriors[\"tau_1_over_T\"].loc[assets].values\n",
    "except NameError:\n",
    "    mu_post = (R.mean() * FREQ).values  # fallback\n",
    "\n",
    "# Bounds (same as before)\n",
    "CAP_SEMI, CAP_GLD_TLT, CAP_VIXY = 0.25, 0.30, 0.05\n",
    "def make_bounds(assets):\n",
    "    b = []\n",
    "    for a in assets:\n",
    "        if a == \"VIXY\":\n",
    "            b.append((0.0, CAP_VIXY))\n",
    "        elif a in (\"GLD\",\"TLT\"):\n",
    "            b.append((0.0, CAP_GLD_TLT))\n",
    "        else:\n",
    "            b.append((0.0, CAP_SEMI))\n",
    "    return tuple(b)\n",
    "bounds = make_bounds(assets)\n",
    "\n",
    "Sigma = Sigma_sample.loc[assets, assets].values\n",
    "\n",
    "def mean_variance(mu, Sigma, delta, bounds):\n",
    "    p = len(mu)\n",
    "    def obj(w): return 0.5*delta*float(w @ Sigma @ w) - float(mu @ w)\n",
    "    cons = [{\"type\":\"eq\", \"fun\": lambda w: np.sum(w) - 1.0}]\n",
    "    x0 = np.full(p, 1.0/p)\n",
    "    return minimize(obj, x0=x0, method=\"SLSQP\", bounds=bounds, constraints=cons,\n",
    "                    options={\"maxiter\": 4000, \"ftol\": 1e-12})\n",
    "\n",
    "def max_sharpe(mu, Sigma, bounds):\n",
    "    p = len(mu)\n",
    "    def shp(w):\n",
    "        num = float(mu @ w); den = np.sqrt(float(w @ Sigma @ w))\n",
    "        return num/(den+1e-16)\n",
    "    def obj(w): return -shp(w)\n",
    "    cons = [{\"type\":\"eq\", \"fun\": lambda w: np.sum(w) - 1.0}]\n",
    "    x0 = np.full(p, 1.0/p)\n",
    "    return minimize(obj, x0=x0, method=\"SLSQP\", bounds=bounds, constraints=cons,\n",
    "                    options={\"maxiter\": 4000, \"ftol\": 1e-12})\n",
    "\n",
    "# Get delta_Market (prefer existing; else compute from posterior Î¼ & Î£)\n",
    "try:\n",
    "    delta_market = float(deltas[\"Market\"])\n",
    "except Exception:\n",
    "    # quick recompute from posterior Î¼ (consistent with our BL view of the world)\n",
    "    w_eq = np.full(len(assets), 1.0/len(assets))  # just to get a scale; any reasonable proxy works\n",
    "    mu_b = float(mu_post @ w_eq)\n",
    "    sig2_b = float(w_eq @ Sigma @ w_eq)\n",
    "    delta_market = (mu_b/sig2_b) if sig2_b>0 else 10.0\n",
    "\n",
    "# Solve weights\n",
    "res_mvo = mean_variance(mu_post, Sigma, delta_market, bounds)\n",
    "res_ms  = max_sharpe(mu_post, Sigma, bounds)\n",
    "if (not res_mvo.success) or (not res_ms.success):\n",
    "    warnings.warn(f\"Optimiser(s) failed. MVO={res_mvo.success}, MS={res_ms.success}\")\n",
    "\n",
    "w_mvo = pd.Series(res_mvo.x, index=assets, name=\"BL_MVO_Market\")\n",
    "w_ms  = pd.Series(res_ms.x,  index=assets, name=\"BL_MaxSharpe\")\n",
    "\n",
    "# Risk contributions\n",
    "def risk_contrib(Sigma, w):\n",
    "    port_sigma = np.sqrt(float(w @ Sigma @ w))\n",
    "    mrc = (Sigma @ w) / (port_sigma + 1e-16)        # marginal risk contribution\n",
    "    prc = (w * mrc) / (port_sigma + 1e-16)          # percentage risk contribution; sums to 1\n",
    "    return port_sigma, mrc, prc\n",
    "\n",
    "vol_mvo, mrc_mvo, prc_mvo = risk_contrib(Sigma, w_mvo.values)\n",
    "vol_ms,  mrc_ms,  prc_ms  = risk_contrib(Sigma, w_ms.values)\n",
    "\n",
    "rc_mvo = pd.DataFrame({\n",
    "    \"% Weight\": w_mvo.values*100,\n",
    "    \"MRC\": mrc_mvo,\n",
    "    \"% Risk\": prc_mvo*100\n",
    "}, index=assets)\n",
    "\n",
    "rc_ms = pd.DataFrame({\n",
    "    \"% Weight\": w_ms.values*100,\n",
    "    \"MRC\": mrc_ms,\n",
    "    \"% Risk\": prc_ms*100\n",
    "}, index=assets)\n",
    "\n",
    "# Write LaTeX tables\n",
    "(OUTDIR / \"step6_rc_MV_sample.tex\").write_text(rc_mvo.to_latex(escape=False, float_format=lambda x: f\"{x: .4f}\"))\n",
    "(OUTDIR / \"step6_rc_MS_sample.tex\").write_text(rc_ms.to_latex(escape=False, float_format=lambda x: f\"{x: .4f}\"))\n",
    "\n",
    "print(f\"MVO portfolio vol (annual): {vol_mvo:.4f}\")\n",
    "print(f\"MS  portfolio vol (annual): {vol_ms:.4f}\")\n",
    "print(\"Wrote: step6_rc_MV_sample.tex, step6_rc_MS_sample.tex\")\n"
   ],
   "id": "fd8d5fca4c0db90b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVO portfolio vol (annual): 0.2922\n",
      "MS  portfolio vol (annual): 0.2603\n",
      "Wrote: step6_rc_MV_sample.tex, step6_rc_MS_sample.tex\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T19:33:13.543600Z",
     "start_time": "2025-08-19T19:33:13.189020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Step 6 â€” Cell 12: NVDA cap = 30% (others unchanged), re-run MVO (Market) & Max Sharpe ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from pathlib import Path\n",
    "\n",
    "OUTDIR = Path(\"outputs\"); OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Reuse from earlier cells: assets, Sigma_sample, posteriors, w_mcap\n",
    "mu_post = posteriors[\"tau_1_over_T\"].loc[assets].values\n",
    "Sigma_mat = Sigma_sample.loc[assets, assets].values\n",
    "\n",
    "# --- Per-asset bounds with NVDA at 30% ---\n",
    "CAP_SEMI, CAP_GLD_TLT, CAP_VIXY = 0.25, 0.30, 0.05\n",
    "def make_bounds_nvda30(assets):\n",
    "    b = []\n",
    "    for a in assets:\n",
    "        if a == \"NVDA\":\n",
    "            b.append((0.0, 0.30))\n",
    "        elif a == \"VIXY\":\n",
    "            b.append((0.0, CAP_VIXY))\n",
    "        elif a in (\"GLD\",\"TLT\"):\n",
    "            b.append((0.0, CAP_GLD_TLT))\n",
    "        else:\n",
    "            b.append((0.0, CAP_SEMI))\n",
    "    return tuple(b)\n",
    "\n",
    "bounds_nvda30 = make_bounds_nvda30(assets)\n",
    "\n",
    "def mean_variance(mu, Sigma, delta, bounds):\n",
    "    p = len(mu)\n",
    "    def obj(w): return 0.5*delta*float(w @ Sigma @ w) - float(mu @ w)\n",
    "    cons = [{\"type\":\"eq\", \"fun\": lambda w: np.sum(w) - 1.0}]\n",
    "    x0 = np.full(p, 1.0/p)\n",
    "    return minimize(obj, x0=x0, method=\"SLSQP\", bounds=bounds, constraints=cons,\n",
    "                    options={\"maxiter\": 4000, \"ftol\": 1e-12})\n",
    "\n",
    "def max_sharpe(mu, Sigma, bounds):\n",
    "    p = len(mu)\n",
    "    def shp(w):\n",
    "        num = float(mu @ w); den = np.sqrt(float(w @ Sigma @ w))\n",
    "        return num/(den+1e-16)\n",
    "    def obj(w): return -shp(w)\n",
    "    cons = [{\"type\":\"eq\", \"fun\": lambda w: np.sum(w) - 1.0}]\n",
    "    x0 = np.full(p, 1.0/p)\n",
    "    return minimize(obj, x0=x0, method=\"SLSQP\", bounds=bounds, constraints=cons,\n",
    "                    options={\"maxiter\": 4000, \"ftol\": 1e-12})\n",
    "\n",
    "# Get delta_Market (from memory if available; else recompute)\n",
    "try:\n",
    "    delta_market = float(deltas[\"Market\"])\n",
    "except Exception:\n",
    "    mu_b = float(mu_post @ w_mcap.loc[assets].values)\n",
    "    sig2_b = float(w_mcap.loc[assets].values @ Sigma_mat @ w_mcap.loc[assets].values)\n",
    "    delta_market = (mu_b/sig2_b) if sig2_b>0 else 10.0\n",
    "\n",
    "# --- Solve with NVDA 30% cap ---\n",
    "res_mvo = mean_variance(mu_post, Sigma_mat, delta_market, bounds_nvda30)\n",
    "res_ms  = max_sharpe(mu_post, Sigma_mat, bounds_nvda30)\n",
    "assert res_mvo.success and res_ms.success, \"Optimisers failed under nvda30 bounds.\"\n",
    "\n",
    "w_mvo30 = pd.Series(res_mvo.x, index=assets, name=\"MVO_Market_nvda30\")\n",
    "w_ms30  = pd.Series(res_ms.x,  index=assets, name=\"MS_Market_nvda30\")\n",
    "\n",
    "# --- Write weights (new filenames) ---\n",
    "(OUTDIR / \"step6_w_MV_sample_Market_nvda30.tex\").write_text(\n",
    "    (w_mvo30*100).to_frame(\"% Weight\").to_latex(escape=False, float_format=lambda x: f\"{x: .2f}\")\n",
    ")\n",
    "(OUTDIR / \"step6_w_MS_sample_Market_nvda30.tex\").write_text(\n",
    "    (w_ms30*100).to_frame(\"% Weight\").to_latex(escape=False, float_format=lambda x: f\"{x: .2f}\")\n",
    ")\n",
    "\n",
    "# --- Active vs MCAP (weight-based) metrics + bar charts ---\n",
    "d_mvo = (w_mvo30 - w_mcap.loc[assets]); d_ms = (w_ms30 - w_mcap.loc[assets])\n",
    "te_mvo = np.sqrt(float(d_mvo.values @ Sigma_mat @ d_mvo.values))\n",
    "te_ms  = np.sqrt(float(d_ms.values  @ Sigma_mat @ d_ms.values))\n",
    "act_mu_mvo = float(mu_post @ d_mvo.values); act_mu_ms = float(mu_post @ d_ms.values)\n",
    "act_sr_mvo = act_mu_mvo/(te_mvo+1e-16);    act_sr_ms  = act_mu_ms /(te_ms +1e-16)\n",
    "\n",
    "tbl = pd.DataFrame({\n",
    "    \"Active TE\": [te_mvo, te_ms],\n",
    "    \"Active ExpRet\": [act_mu_mvo, act_mu_ms],\n",
    "    \"Active Sharpe\": [act_sr_mvo, act_sr_ms],\n",
    "}, index=[\"MVO_Market_nvda30\", \"MS_Market_nvda30\"])\n",
    "(OUTDIR / \"step6_active_vs_mcap_nvda30.tex\").write_text(\n",
    "    tbl.to_latex(escape=False, float_format=lambda x: f\"{x: .4f}\")\n",
    ")\n",
    "\n",
    "# Active bars\n",
    "for name, active in [(\"MVO_Market_nvda30\", d_mvo), (\"MS_Market_nvda30\", d_ms)]:\n",
    "    plt.figure(figsize=(9,3.4))\n",
    "    plt.bar(active.index, active.values)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.ylabel(\"Active Weight\")\n",
    "    plt.title(f\"Active Weights vs MCAP: {name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTDIR / f\"step6_active_{name}.png\"); plt.close()\n",
    "\n",
    "print(\"Wrote: step6_w_MV_sample_Market_nvda30.tex, step6_w_MS_sample_Market_nvda30.tex, step6_active_vs_mcap_nvda30.tex, step6_active_MVO_Market_nvda30.png, step6_active_MS_Market_nvda30.png\")\n"
   ],
   "id": "342e9915b1784913",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: step6_w_MV_sample_Market_nvda30.tex, step6_w_MS_sample_Market_nvda30.tex, step6_active_vs_mcap_nvda30.tex, step6_active_MVO_Market_nvda30.png, step6_active_MS_Market_nvda30.png\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T19:33:52.046898Z",
     "start_time": "2025-08-19T19:33:51.582963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Step 6 â€” Cell 13: Active vs SMH (return-based) for nvda30 portfolios ===\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "OUTDIR = Path(\"outputs\"); OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "RET_FILE = OUTDIR / \"step2_excess_returns.csv\"\n",
    "\n",
    "ret_all = pd.read_csv(RET_FILE, index_col=0, parse_dates=True)\n",
    "assets = [c for c in ret_all.columns if c in [\"NVDA\",\"AMD\",\"ASML\",\"AMAT\",\"TSM\",\"LRCX\",\"INTC\",\"GLD\",\"TLT\",\"VIXY\"]]\n",
    "R = ret_all[assets].copy()\n",
    "FREQ = 252\n",
    "\n",
    "# Need w_mvo30 and w_ms30; if not present, stop.\n",
    "try:\n",
    "    w_mvo30\n",
    "    w_ms30\n",
    "except NameError:\n",
    "    raise RuntimeError(\"Run Cell 12 first to compute nvda30 weights.\")\n",
    "\n",
    "# Build portfolio daily excess returns (fixed weights)\n",
    "p_mvo = (R @ w_mvo30).rename(\"BL_MVO_nvda30\")\n",
    "p_ms  = (R @ w_ms30).rename(\"BL_MS_nvda30\")\n",
    "\n",
    "# SMH excess series\n",
    "if \"SMH\" not in ret_all.columns:\n",
    "    print(\"SMH not found in returns file; skipping SMH active metrics.\")\n",
    "else:\n",
    "    smh = ret_all[\"SMH\"]\n",
    "    active_mvo = (p_mvo - smh).dropna()\n",
    "    active_ms  = (p_ms  - smh).dropna()\n",
    "\n",
    "    # Annualized TE and IR (return-based)\n",
    "    te_mvo = active_mvo.std()*np.sqrt(FREQ)\n",
    "    te_ms  = active_ms.std()*np.sqrt(FREQ)\n",
    "    ir_mvo = active_mvo.mean()*FREQ / (te_mvo + 1e-16)\n",
    "    ir_ms  = active_ms.mean()*FREQ / (te_ms  + 1e-16)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"TE (ann)\": [te_mvo, te_ms],\n",
    "        \"IR\":       [ir_mvo, ir_ms],\n",
    "        \"Ann Active Ret (%)\": [active_mvo.mean()*FREQ*100, active_ms.mean()*FREQ*100],\n",
    "    }, index=[\"BL_MVO_nvda30\",\"BL_MS_nvda30\"])\n",
    "\n",
    "    (OUTDIR / \"step6_active_vs_smh_nvda30.tex\").write_text(\n",
    "        df.to_latex(escape=False, float_format=lambda x: f\"{x: .3f}\")\n",
    "    )\n",
    "\n",
    "    # cumulative active P&L\n",
    "    plt.figure(figsize=(8.0,4.0))\n",
    "    (1+active_mvo).cumprod().plot(label=\"BL_MVO_nvda30 - SMH\")\n",
    "    (1+active_ms).cumprod().plot(label=\"BL_MS_nvda30 - SMH\")\n",
    "    plt.title(\"Cumulative Active P&L vs SMH (No Rebalancing)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTDIR / \"step6_active_cum_vs_smh_nvda30.png\"); plt.close()\n",
    "\n",
    "    print(\"Wrote: step6_active_vs_smh_nvda30.tex, step6_active_cum_vs_smh_nvda30.png\")\n"
   ],
   "id": "e9ce4f1f1ae73fbe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: step6_active_vs_smh_nvda30.tex, step6_active_cum_vs_smh_nvda30.png\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T20:29:02.967535Z",
     "start_time": "2025-08-19T20:28:57.944459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nbformat\n",
    "from pathlib import Path\n",
    "\n",
    "nb_path = Path(\"/Users/dave/PythonProject1/PythonProject1/PythonProject/BlackLitterman/BlackLitterman.ipynb\")\n",
    "nb = nbformat.read(nb_path, as_version=4)\n",
    "\n",
    "raw = nonempty = code_only = 0\n",
    "for cell in nb.cells:\n",
    "    if cell.cell_type != \"code\":\n",
    "        continue\n",
    "    for line in cell.source.splitlines():\n",
    "        raw += 1\n",
    "        s = line.strip()\n",
    "        if s:\n",
    "            nonempty += 1\n",
    "        if s and not s.startswith(('#','%','!')):\n",
    "            code_only += 1\n",
    "\n",
    "print(f\"Notebook: {nb_path.name}\")\n",
    "print(f\" Raw lines   : {raw}\")\n",
    "print(f\" Non-empty   : {nonempty}\")\n",
    "print(f\" Code only   : {code_only}\")\n"
   ],
   "id": "13c8453d2a237059",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook: BlackLitterman.ipynb\n",
      " Raw lines   : 2376\n",
      " Non-empty   : 2026\n",
      " Code only   : 1748\n"
     ]
    }
   ],
   "execution_count": 27
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
